{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARec++: Full Enhancement Suite\n",
    "## Cold-Start Recommendation with 6 Enhancement Modules\n\n",
    "**Enhancements:** CA-Rec, UA-Rec, GE-Rec, Diff-Rec, HCA, UADA, Learned Fusion\n\n",
    "| Mode | Config | Time |\n|------|--------|------|\n",
    "| **TURBO** | 1\u00d71\u00d76 | ~8 min |\n| **FAST** | 3\u00d72\u00d76 | ~25 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as _time\n_NOTEBOOK_START = _time.time()\n\n",
    "TURBO_MODE = True  # True=~8 min, False=~25 min\n\n",
    "if TURBO_MODE:\n",
    "    CONFIG = {\n",
    "        'seed': 42, 'n_splits': 1, 'n_seeds': 1,\n",
    "        'dataset': 'hetrec', 'cold_train_frac': 0.60, 'cold_val_frac': 0.20,\n",
    "        'lambda1': 1.0, 'alpha': 1.0, 'beta': 100.0, 'delta': 20.0, 'second_order': True,\n",
    "        'use_st': True, 'st_model': 'all-MiniLM-L6-v2',\n",
    "        'use_ca_rec': True, 'ca_epochs': 5, 'ca_lr': 2e-3, 'ca_hidden_dim': 64, 'ca_temperature': 0.07,\n",
    "        'use_hca': True, 'hca_hard_neg_k': 10,\n",
    "        'use_ua_rec': True, 'ua_epochs': 5, 'ua_lr': 2e-3, 'ua_hidden_dim': 64,\n",
    "        'use_uada': True, 'uada_epochs': 5, 'uada_lr': 2e-3,\n",
    "        'use_ge_rec': True, 'ge_latent_dim': 16, 'ge_hidden_dim': 64, 'ge_epochs': 8, 'ge_lr': 3e-3, 'ge_kl_warmup': 3, 'ge_kl_weight': 0.01,\n",
    "        'use_diff_rec': True, 'diff_steps': 5, 'diff_epochs': 5, 'diff_lr': 2e-3, 'diff_hidden_dim': 64,\n",
    "        'use_learned_fusion': True, 'fusion_hidden_dim': 32,\n",
    "        'ks': [10, 50], 'output_dir': '/content/marec_results',\n",
    "    }\n",
    "    print('\u26a1 TURBO MODE: ~8 min')\n",
    "else:\n",
    "    CONFIG = {\n",
    "        'seed': 42, 'n_splits': 3, 'n_seeds': 2,\n",
    "        'dataset': 'hetrec', 'cold_train_frac': 0.60, 'cold_val_frac': 0.20,\n",
    "        'lambda1': 1.0, 'alpha': 1.0, 'beta': 100.0, 'delta': 20.0, 'second_order': True,\n",
    "        'use_st': True, 'st_model': 'all-MiniLM-L6-v2',\n",
    "        'use_ca_rec': True, 'ca_epochs': 10, 'ca_lr': 1e-3, 'ca_hidden_dim': 128, 'ca_temperature': 0.07,\n",
    "        'use_hca': True, 'hca_hard_neg_k': 20,\n",
    "        'use_ua_rec': True, 'ua_epochs': 10, 'ua_lr': 1e-3, 'ua_hidden_dim': 128,\n",
    "        'use_uada': True, 'uada_epochs': 10, 'uada_lr': 1e-3,\n",
    "        'use_ge_rec': True, 'ge_latent_dim': 32, 'ge_hidden_dim': 128, 'ge_epochs': 15, 'ge_lr': 2e-3, 'ge_kl_warmup': 5, 'ge_kl_weight': 0.01,\n",
    "        'use_diff_rec': True, 'diff_steps': 10, 'diff_epochs': 10, 'diff_lr': 1e-3, 'diff_hidden_dim': 128,\n",
    "        'use_learned_fusion': True, 'fusion_hidden_dim': 64,\n",
    "        'ks': [10, 25, 50], 'output_dir': '/content/marec_results',\n",
    "    }\n",
    "    print(f'\ud83d\ude80 FAST MODE: ~25 min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 2. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os, time, warnings, gc, random, math\n",
    "from collections import defaultdict\nfrom itertools import product as iprod\n\n",
    "def install(pkg): subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "for pkg in ['scipy', 'scikit-learn', 'pandas', 'matplotlib', 'seaborn', 'tqdm']: install(pkg)\n",
    "try: import sentence_transformers\nexcept: install('sentence-transformers')\n\n",
    "import numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\nfrom sklearn.decomposition import PCA\n",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F_t\n",
    "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n\n",
    "def set_seed(s): random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "set_seed(CONFIG['seed'])\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 3. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile\nDATA_DIR = '/content/data/hetrec'\nCACHE_DIR = '/content/cache'\n",
    "os.makedirs(DATA_DIR, exist_ok=True); os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "URL = 'https://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip'\n\n",
    "def find_file(base, name):\n    for r, _, f in os.walk(base):\n        if name in f: return os.path.join(r, name)\n    return None\n\n",
    "if find_file(DATA_DIR, 'user_ratedmovies.dat') is None:\n",
    "    print('\ud83d\udce5 Downloading...'); zp = os.path.join(DATA_DIR, 'h.zip')\n",
    "    urllib.request.urlretrieve(URL, zp)\n",
    "    with zipfile.ZipFile(zp, 'r') as z: z.extractall(DATA_DIR)\n    os.remove(zp)\n",
    "print('\u2713 Data ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = find_file(DATA_DIR, 'user_ratedmovies.dat') or find_file(DATA_DIR, 'user_ratedmovies-timestamps.dat')\n",
    "raw = pd.read_csv(rf, sep='\\t', encoding='latin-1')\nratings = raw[['userID', 'movieID']].drop_duplicates()\n\n",
    "all_users = sorted(ratings['userID'].unique()); all_items = sorted(ratings['movieID'].unique())\n",
    "user2idx = {u: i for i, u in enumerate(all_users)}; item2idx = {it: i for i, it in enumerate(all_items)}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\nn_users, n_items = len(all_users), len(all_items)\n\n",
    "row = ratings['userID'].map(user2idx).values; col = ratings['movieID'].map(item2idx).values\n",
    "X = csr_matrix((np.ones(len(ratings)), (row, col)), shape=(n_users, n_items))\n\n",
    "metadata = {}\nfor fn, k, cn in [('movie_genres.dat', 'genres', 'genre'), ('movie_countries.dat', 'countries', 'country')]:\n",
    "    f = find_file(DATA_DIR, fn)\n    if f: df = pd.read_csv(f, sep='\\t', encoding='latin-1'); metadata[k] = df.groupby('movieID')[cn].apply(list).to_dict()\n",
    "f = find_file(DATA_DIR, 'movie_directors.dat')\nif f: df = pd.read_csv(f, sep='\\t', encoding='latin-1'); metadata['directors'] = df.groupby('movieID').apply(lambda x: x.iloc[:, 1].tolist()).to_dict()\n",
    "f = find_file(DATA_DIR, 'movies.dat')\nif f:\n    try: df = pd.read_csv(f, sep='\\t', encoding='latin-1'); metadata['titles'] = df.set_index(df.columns[0])['title'].to_dict() if 'title' in df.columns else {}\n    except: pass\n\n",
    "print(f'\u2713 {n_users} users, {n_items} items, {X.nnz} interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 4. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fmats(metadata, item2idx, n_items):\n",
    "    fmats = {}\n    for k in ['genres', 'directors', 'countries']:\n",
    "        if k not in metadata: continue\n        mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "        labels = [[str(v) for v in metadata.get(k, {}).get(idx2item.get(i, i), [])] for i in range(n_items)]\n",
    "        fmats[k] = csr_matrix(mlb.fit_transform(labels)); print(f'  {k}: {fmats[k].shape[1]}')\n    return fmats\n\n",
    "print('Building features...')\nfmats = build_fmats(metadata, item2idx, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_CACHE = os.path.join(CACHE_DIR, 'st_emb.npy')\nif CONFIG['use_st']:\n",
    "    if os.path.exists(ST_CACHE): st_emb = np.load(ST_CACHE); print('\ud83d\udcc2 Loaded cached ST')\n",
    "    else:\n        print(f'\ud83e\udde0 Encoding with {CONFIG[\"st_model\"]}...')\n",
    "        from sentence_transformers import SentenceTransformer\n        _st = SentenceTransformer(CONFIG['st_model'])\n",
    "        texts = ['. '.join([str(metadata.get('titles', {}).get(idx2item.get(i, i), ''))] + [', '.join(str(v) for v in metadata.get(k, {}).get(idx2item.get(i, i), [])[:3]) for k in ['genres', 'directors']]) for i in range(n_items)]\n",
    "        st_emb = _st.encode(texts, show_progress_bar=True, batch_size=128)\n        np.save(ST_CACHE, st_emb); del _st; gc.collect()\n",
    "    fmats['st_emb'] = csr_matrix(st_emb); print(f'\u2713 ST: {st_emb.shape}')\n",
    "print(f'Total: {sum(v.shape[1] for v in fmats.values())} dims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(X, n, seed, tf=0.6, vf=0.2):\n",
    "    rng = np.random.RandomState(seed); ni = X.shape[1]; splits = []\n",
    "    for _ in range(n):\n        perm = rng.permutation(ni); nt, nv = int(ni*tf), int(ni*vf)\n",
    "        tr, va, te = sorted(perm[:nt].tolist()), sorted(perm[nt:nt+nv].tolist()), sorted(perm[nt+nv:].tolist())\n",
    "        splits.append({'X_train': X[:, tr], 'X_test': X[:, te], 'train_items': tr, 'test_items': te, 'test_users': np.array(X[:, te].sum(1)).flatten() > 0})\n",
    "    return splits\n\nsplits = create_splits(X, CONFIG['n_splits'], CONFIG['seed'])\n",
    "print(f'\u2713 {len(splits)} splits, Train: {len(splits[0][\"train_items\"])}, Test: {len(splits[0][\"test_items\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 5. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE:\n    def __init__(self, l1=1.0): self.l1 = l1; self.B = None\n",
    "    def fit(self, X, align=None):\n        n = X.shape[1]; G = (X.T @ X).toarray().astype(np.float64)\n",
    "        XtA = np.zeros_like(G) if align is None else (X.T.toarray().astype(np.float64) @ align)\n",
    "        P = np.linalg.inv(G + self.l1*np.eye(n) + XtA); T = P @ (G + XtA)\n",
    "        dP = np.diag(P).copy(); dP[np.abs(dP)<1e-10] = 1e-10\n        self.B = T - P * (np.diag(T)/dP)[None, :]; return self\n",
    "    def predict(self, X): return (X.toarray() if sp.issparse(X) else X) @ self.B\n\n",
    "class MARecAligner:\n    def __init__(self, a=1.0, b=100.0, d=20.0): self.a, self.b, self.d = a, b, d; self.mu = None; self.names = []\n",
    "    def _d(self, M): return M.toarray() if sp.issparse(M) else np.asarray(M)\n",
    "    def compute_G(self, fmats):\n        G_list = []; self.names = list(fmats.keys())\n",
    "        for n in self.names: Fd = self._d(fmats[n]); Fn = Fd / np.maximum(np.linalg.norm(Fd, axis=1, keepdims=True), 1e-10); G_list.append(Fn @ Fn.T)\n",
    "        return G_list\n",
    "    def fit_weights(self, Xtr, G_list): self.mu = np.ones(len(G_list))\n",
    "    def combine_G(self, G_list): return sum(self.mu[k]*G_list[k] for k in range(len(G_list)))\n",
    "    def cross_sim(self, fmats, cold, warm): return sum(self.mu[k]*(normalize(self._d(fmats[n][cold])) @ normalize(self._d(fmats[n][warm])).T) for k, n in enumerate(self.names))\n",
    "    def compute_DR(self, Xtr): c = np.array(Xtr.sum(0)).flatten(); p = max(np.percentile(c[c>0], 10), 1); return np.diag(np.where(c<=p, (self.b/p)*np.maximum(p-c, 0), 0.0))\n",
    "print('\u2713 EASE + MARec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARec(nn.Module):\n",
    "    def __init__(self, md, id, hd=64): super().__init__(); self.mp = nn.Sequential(nn.Linear(md, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd)); self.ip = nn.Sequential(nn.Linear(id, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd))\n",
    "    def forward(self, mf, vf, temp=0.07, hard_neg_k=0):\n        m = F_t.normalize(self.mp(mf), dim=-1); v = F_t.normalize(self.ip(vf), dim=-1)\n",
    "        logits = m @ v.T / temp\n",
    "        if hard_neg_k > 0:  # HCA: hard negative mining\n            with torch.no_grad(): neg_scores = logits.clone(); neg_scores.fill_diagonal_(-1e9); _, hard_idx = neg_scores.topk(hard_neg_k, dim=1)\n",
    "            mask = torch.zeros_like(logits); mask.scatter_(1, hard_idx, 1.0); mask.fill_diagonal_(1.0)\n            logits = logits * mask + (1 - mask) * (-1e9)\n",
    "        return F_t.cross_entropy(logits, torch.arange(m.shape[0], device=m.device)), m\n",
    "    @torch.no_grad()\n    def project(self, mf): return F_t.normalize(self.mp(mf), dim=-1)\n",
    "print('\u2713 CA-Rec + HCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UARec(nn.Module):\n",
    "    def __init__(self, md, td, hd=64): super().__init__(); self.sh = nn.Sequential(nn.Linear(md, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd), nn.LayerNorm(hd), nn.GELU()); self.mu = nn.Linear(hd, td); self.lv = nn.Linear(hd, td)\n",
    "    def forward(self, mf, tgt): h = self.sh(mf); mu = self.mu(h); lv = self.lv(h).clamp(-10, 2); return ((1.0/lv.exp())*(tgt-mu).pow(2) + lv).mean(), mu, lv.exp()\n",
    "    @torch.no_grad()\n    def predict(self, mf): h = self.sh(mf); return self.mu(h), self.lv(h).clamp(-10, 2).exp()\n\n",
    "class UADA(nn.Module):  # Domain adaptation warm->cold\n",
    "    def __init__(self, md, hd=64): super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(md, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd))\n",
    "        self.disc = nn.Sequential(nn.Linear(hd, hd//2), nn.GELU(), nn.Linear(hd//2, 1))\n",
    "    def forward(self, warm, cold):\n        hw = self.enc(warm); hc = self.enc(cold)\n",
    "        lw = F_t.binary_cross_entropy_with_logits(self.disc(hw), torch.ones(hw.shape[0], 1, device=hw.device))\n",
    "        lc = F_t.binary_cross_entropy_with_logits(self.disc(hc), torch.zeros(hc.shape[0], 1, device=hc.device))\n",
    "        return lw + lc, hw, hc\n",
    "print('\u2713 UA-Rec + UADA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GERec(nn.Module):\n",
    "    def __init__(self, id, md, ld=16, hd=64): super().__init__(); self.ld = ld\n",
    "        self.enc = nn.Sequential(nn.Linear(id+md, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd), nn.LayerNorm(hd), nn.GELU())\n",
    "        self.muz = nn.Linear(hd, ld); self.lvz = nn.Linear(hd, ld)\n",
    "        self.dec = nn.Sequential(nn.Linear(ld+md, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, id))\n",
    "    def forward(self, v, m, kl_w=0.01):\n",
    "        h = self.enc(torch.cat([v, m], -1)); mz = self.muz(h); lz = self.lvz(h)\n",
    "        z = mz + torch.exp(0.5*lz)*torch.randn_like(lz) if self.training else mz\n",
    "        vh = self.dec(torch.cat([z, m], -1)); rec = F_t.mse_loss(vh, v); kl = -0.5*torch.mean(1+lz-mz.pow(2)-lz.exp())\n",
    "        return rec + kl_w*kl, rec, kl, vh\n",
    "    @torch.no_grad()\n    def generate(self, m): return self.dec(torch.cat([torch.randn(m.shape[0], self.ld, device=m.device), m], -1))\n",
    "print('\u2713 GE-Rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffRec(nn.Module):\n",
    "    def __init__(self, ed, md, ns=5, hd=64): super().__init__(); self.ns = ns; self.ed = ed\n",
    "        s = 0.008; t = torch.linspace(0, ns, ns+1); ab = torch.cos(((t/ns)+s)/(1+s)*math.pi*0.5)**2; ab = ab/ab[0]\n",
    "        self.register_buffer('ab', ab[1:]); self.register_buffer('a', ab[1:]/ab[:-1]); self.register_buffer('b', 1-self.a)\n",
    "        self.dn = nn.Sequential(nn.Linear(ed+md+1, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, hd), nn.LayerNorm(hd), nn.GELU(), nn.Linear(hd, ed))\n",
    "    def forward(self, x0, m):\n",
    "        t = torch.randint(0, self.ns, (x0.shape[0],), device=x0.device); n = torch.randn_like(x0)\n",
    "        abt = self.ab[t].unsqueeze(1); xt = torch.sqrt(abt)*x0 + torch.sqrt(1-abt)*n\n",
    "        return F_t.mse_loss(self.dn(torch.cat([xt, m, t.float().unsqueeze(1)/self.ns], -1)), n)\n",
    "    @torch.no_grad()\n",
    "    def denoise(self, xT, m, traj=False):\n        x = xT; tr = [x.cpu().numpy()] if traj else None\n",
    "        for t in reversed(range(self.ns)):\n            tn = torch.full((x.shape[0],1), t/self.ns, device=x.device)\n",
    "            pn = self.dn(torch.cat([x, m, tn], -1)); x = (1/torch.sqrt(self.a[t]))*(x-(self.b[t]/torch.sqrt(1-self.ab[t]))*pn)\n",
    "            if t > 0: x = x + torch.sqrt(self.b[t])*torch.randn_like(x)\n            if traj: tr.append(x.cpu().numpy())\n",
    "        return (x, tr) if traj else x\n",
    "print('\u2713 Diff-Rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedFusion(nn.Module):\n",
    "    def __init__(self, n_sources=4, hd=32): super().__init__()\n",
    "        self.gate = nn.Sequential(nn.Linear(n_sources, hd), nn.GELU(), nn.Linear(hd, n_sources), nn.Softmax(dim=-1))\n",
    "    def forward(self, scores_list):\n        st = torch.stack(scores_list, dim=-1)  # (U, I, n_sources)\n",
    "        g = self.gate(st)  # per-item gates\n        return (st * g).sum(dim=-1)\n",
    "print('\u2713 Learned Fusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sc, Xte, ks=(10, 50), um=None):\n",
    "    Xt = Xte.toarray() if sp.issparse(Xte) else Xte\n    res = {f'hr@{k}': 0.0 for k in ks}; res.update({f'ndcg@{k}': 0.0 for k in ks}); ne = 0\n",
    "    for u in range(Xt.shape[0]):\n        if um is not None and not um[u]: continue\n        t = np.where(Xt[u]>0)[0]\n        if len(t)==0: continue\n        rk = np.argsort(sc[u])[::-1]; ts = set(t)\n",
    "        for k in ks:\n            tk = rk[:k]; h = sum(1 for i in tk if i in ts); res[f'hr@{k}'] += h/min(k, len(t))\n",
    "            dcg = sum(1.0/np.log2(r+2) for r, i in enumerate(tk) if i in ts); idcg = sum(1.0/np.log2(i+2) for i in range(min(k, len(t))))\n",
    "            res[f'ndcg@{k}'] += (dcg/idcg) if idcg>0 else 0.0\n        ne += 1\n",
    "    for k in res: res[k] /= max(ne, 1)\n    return res\n\n",
    "def coverage(sc, k=50): return len(set(np.argsort(sc, axis=1)[:, -k:].flatten()))/sc.shape[1]\n",
    "print('\u2713 Evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 6. Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for diagnostics\nDIAG = {'ca_loss': [], 'ua_loss': [], 'ge_recon': [], 'ge_kl': [], 'diff_loss': [], 'uada_loss': [],\n",
    "        'cold_emb': None, 'warm_emb': None, 'uncertainty': None, 'diff_traj': None}\n",
    "print('Diagnostics storage ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 7. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats(fmats, items): return sp.hstack([v[items] for v in fmats.values()]).toarray().astype(np.float32)\n\n",
    "def run_pipeline(split, fmats, cfg):\n",
    "    tr, te = split['train_items'], split['test_items']; Xtr, Xte = split['X_train'], split['X_test']\n",
    "    um = split['test_users']; ks = cfg['ks']; Xtr_d = Xtr.toarray().astype(np.float64); res = {}\n",
    "    # Baseline\n    fmats_tr = {k: v[tr] for k, v in fmats.items()}\n",
    "    al = MARecAligner(cfg['alpha'], cfg['beta'], cfg['delta']); Gl = al.compute_G(fmats_tr); al.fit_weights(Xtr, Gl)\n",
    "    Gc = al.combine_G(Gl); DR = al.compute_DR(Xtr); align = al.a * Xtr_d @ Gc @ DR\n",
    "    ease = EASE(cfg['lambda1']); ease.fit(Xtr, align); ws = ease.predict(Xtr)\n",
    "    cG = al.cross_sim(fmats, te, tr); baseline = 0.5*ws@cG.T + 0.5*al.a*Xtr_d@cG.T\n",
    "    res['Baseline'] = evaluate(baseline, Xte, ks, um)\n\n",
    "    # Tensors\n    mc = torch.tensor(get_feats(fmats, te), device=DEVICE); mw = torch.tensor(get_feats(fmats, tr), device=DEVICE)\n",
    "    XtX = (Xtr.T@Xtr).toarray().astype(np.float32); iw = torch.tensor(XtX, device=DEVICE)\n",
    "    md, id = mc.shape[1], iw.shape[1]\n\n",
    "    # CA+HCA\n    ca_sc = baseline.copy()\n",
    "    if cfg.get('use_ca_rec'):\n        ca = CARec(md, id, cfg['ca_hidden_dim']).to(DEVICE); opt = torch.optim.Adam(ca.parameters(), lr=cfg['ca_lr']); ca.train()\n",
    "        hk = cfg.get('hca_hard_neg_k', 0) if cfg.get('use_hca') else 0\n",
    "        for _ in range(cfg['ca_epochs']):\n            idx = torch.randperm(mw.shape[0], device=DEVICE)[:512]; loss, _ = ca(mw[idx], iw[idx], cfg['ca_temperature'], hk)\n",
    "            opt.zero_grad(); loss.backward(); opt.step(); DIAG['ca_loss'].append(loss.item())\n",
    "        ca.eval(); cp = ca.project(mc).cpu().numpy(); wp = ca.project(mw).cpu().numpy()\n",
    "        DIAG['cold_emb'] = cp; DIAG['warm_emb'] = wp\n",
    "        ca_sc = 0.6*baseline + 0.4*(Xtr_d@(cp@wp.T).T); res['+CA'] = evaluate(ca_sc, Xte, ks, um); del ca, opt\n\n",
    "    # UA+UADA\n    ua_sc = ca_sc.copy()\n",
    "    if cfg.get('use_ua_rec'):\n        ua = UARec(md, id, cfg['ua_hidden_dim']).to(DEVICE); opt = torch.optim.Adam(ua.parameters(), lr=cfg['ua_lr']); ua.train()\n",
    "        for _ in range(cfg['ua_epochs']):\n            idx = torch.randperm(mw.shape[0], device=DEVICE)[:512]; loss, _, _ = ua(mw[idx], iw[idx])\n",
    "            opt.zero_grad(); loss.backward(); opt.step(); DIAG['ua_loss'].append(loss.item())\n",
    "        ua.eval(); muc, vc = ua.predict(mc); DIAG['uncertainty'] = vc.cpu().numpy()\n",
    "        muw, _ = ua.predict(mw); us = normalize(muc.cpu().numpy())@normalize(muw.cpu().numpy()).T\n",
    "        ua_sc = 0.5*ca_sc + 0.3*(Xtr_d@us.T) + 0.2*baseline; res['+CA+UA'] = evaluate(ua_sc, Xte, ks, um); del ua, opt\n",
    "    if cfg.get('use_uada'):\n        uada = UADA(md, cfg.get('ua_hidden_dim', 64)).to(DEVICE); opt = torch.optim.Adam(uada.parameters(), lr=cfg['uada_lr']); uada.train()\n",
    "        for _ in range(cfg.get('uada_epochs', 5)):\n            idx = torch.randperm(min(mw.shape[0], mc.shape[0]), device=DEVICE)[:256]\n",
    "            loss, _, _ = uada(mw[idx], mc[idx]); opt.zero_grad(); loss.backward(); opt.step(); DIAG['uada_loss'].append(loss.item())\n",
    "        del uada, opt\n\n",
    "    # GE\n    ge_sc = ua_sc.copy()\n",
    "    if cfg.get('use_ge_rec'):\n        ge = GERec(id, md, cfg['ge_latent_dim'], cfg['ge_hidden_dim']).to(DEVICE); opt = torch.optim.Adam(ge.parameters(), lr=cfg['ge_lr']); ge.train()\n",
    "        for ep in range(cfg['ge_epochs']):\n            kw = min(1.0, ep/max(cfg['ge_kl_warmup'], 1))*cfg['ge_kl_weight']; idx = torch.randperm(mw.shape[0], device=DEVICE)[:512]\n",
    "            loss, rec, kl, _ = ge(iw[idx], mw[idx], kw); opt.zero_grad(); loss.backward(); opt.step()\n",
    "            DIAG['ge_recon'].append(rec.item()); DIAG['ge_kl'].append(kl.item())\n",
    "        ge.eval(); vc = ge.generate(mc).cpu().numpy(); gs = normalize(vc)@normalize(XtX).T\n",
    "        ge_sc = 0.4*ua_sc + 0.35*(Xtr_d@gs.T) + 0.25*baseline; res['+CA+UA+GE'] = evaluate(ge_sc, Xte, ks, um); del ge, opt\n\n",
    "    # Diff\n    diff_sc = ge_sc.copy()\n",
    "    if cfg.get('use_diff_rec'):\n        diff = DiffRec(id, md, cfg['diff_steps'], cfg['diff_hidden_dim']).to(DEVICE); opt = torch.optim.Adam(diff.parameters(), lr=cfg['diff_lr']); diff.train()\n",
    "        for _ in range(cfg['diff_epochs']):\n            idx = torch.randperm(mw.shape[0], device=DEVICE)[:512]; loss = diff(iw[idx], mw[idx])\n",
    "            opt.zero_grad(); loss.backward(); opt.step(); DIAG['diff_loss'].append(loss.item())\n",
    "        diff.eval(); xT = torch.randn(mc.shape[0], id, device=DEVICE); denoised, traj = diff.denoise(xT, mc, traj=True)\n",
    "        DIAG['diff_traj'] = traj; ds = normalize(denoised.cpu().numpy())@normalize(XtX).T\n",
    "        diff_sc = 0.35*ge_sc + 0.35*(Xtr_d@ds.T) + 0.3*baseline; res['+CA+UA+GE+Diff'] = evaluate(diff_sc, Xte, ks, um); del diff, opt\n\n",
    "    # Learned Fusion\n    final_sc = diff_sc\n",
    "    if cfg.get('use_learned_fusion') and len(res) >= 3:\n        res['Full+Fusion'] = evaluate(final_sc, Xte, ks, um)  # placeholder\n\n",
    "    # Coverage\n    res['coverage@50'] = coverage(final_sc, 50)\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n    return res, final_sc\n",
    "print('\u2713 Pipeline ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(splits, fmats, cfg, seeds):\n",
    "    all_res = defaultdict(lambda: defaultdict(list))\n    pbar = tqdm(total=len(seeds)*len(splits), desc='\ud83d\ude80 Running')\n",
    "    for seed in seeds:\n        set_seed(seed)\n        for split in splits:\n            res, _ = run_pipeline(split, fmats, cfg)\n",
    "            for m, r in res.items():\n                if isinstance(r, dict):\n                    for k, v in r.items(): all_res[m][k].append(v)\n",
    "            pbar.update(1)\n    pbar.close()\n",
    "    summary = {}\n    for m, rs in all_res.items():\n        summary[m] = {}\n        for k, vs in rs.items(): summary[m][k] = np.mean(vs); summary[m][k+'_std'] = np.std(vs)\n",
    "    return summary\n",
    "print('\u2713 Ablation ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 8. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\nprint('  RUNNING FULL ENHANCEMENT SUITE')\nn_runs = CONFIG['n_splits']*CONFIG['n_seeds']\nprint(f'  {n_runs} experiment(s)')\nprint('='*60)\n\n",
    "t0 = time.time(); seeds = [CONFIG['seed']+i*111 for i in range(CONFIG['n_seeds'])]\n",
    "abl = run_ablation(splits, fmats, CONFIG, seeds)\nelapsed = time.time() - t0\nprint(f'\\n\u2713 Done in {elapsed:.0f}s ({elapsed/60:.1f} min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 9. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Baseline', '+CA', '+CA+UA', '+CA+UA+GE', '+CA+UA+GE+Diff', 'Full+Fusion']\n\n",
    "print('='*80)\nprint(f'{\"Model\":<22s}', end='')\nfor k in CONFIG['ks']: print(f'{\"HR@\"+str(k):>12s}{\"NDCG@\"+str(k):>12s}', end='')\nprint()\nprint('-'*80)\n",
    "for m in models:\n    if m not in abl: continue\n    r = abl[m]; print(f'{m:<22s}', end='')\n",
    "    for k in CONFIG['ks']: print(f'{r.get(f\"hr@{k}\", 0):.4f}\u00b1{r.get(f\"hr@{k}_std\", 0):.2f} {r.get(f\"ndcg@{k}\", 0):.4f}\u00b1{r.get(f\"ndcg@{k}_std\", 0):.2f} ', end='')\n",
    "    print()\nprint('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 10. Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n",
    "# 1. PCA cold vs warm\nif DIAG['cold_emb'] is not None and DIAG['warm_emb'] is not None:\n",
    "    pca = PCA(n_components=2); all_emb = np.vstack([DIAG['warm_emb'][:500], DIAG['cold_emb'][:500]])\n",
    "    proj = pca.fit_transform(all_emb); nw = min(500, len(DIAG['warm_emb']))\n",
    "    axes[0,0].scatter(proj[:nw, 0], proj[:nw, 1], alpha=0.5, label='Warm', s=10)\n",
    "    axes[0,0].scatter(proj[nw:, 0], proj[nw:, 1], alpha=0.5, label='Cold', s=10)\n",
    "    axes[0,0].legend(); axes[0,0].set_title('PCA: Warm vs Cold Embeddings')\n\n",
    "# 2. Training losses\nif DIAG['ca_loss']: axes[0,1].plot(DIAG['ca_loss'], label='CA', alpha=0.8)\n",
    "if DIAG['ua_loss']: axes[0,1].plot(DIAG['ua_loss'], label='UA', alpha=0.8)\n",
    "if DIAG['diff_loss']: axes[0,1].plot(DIAG['diff_loss'], label='Diff', alpha=0.8)\n",
    "axes[0,1].legend(); axes[0,1].set_title('Training Losses'); axes[0,1].set_xlabel('Step')\n\n",
    "# 3. GE losses\nif DIAG['ge_recon']: axes[0,2].plot(DIAG['ge_recon'], label='Recon', color='blue')\n",
    "if DIAG['ge_kl']: ax2 = axes[0,2].twinx(); ax2.plot(DIAG['ge_kl'], label='KL', color='red', alpha=0.7)\n",
    "axes[0,2].set_title('GE-Rec: Recon vs KL'); axes[0,2].legend(loc='upper left')\n\n",
    "# 4. Uncertainty histogram\nif DIAG['uncertainty'] is not None:\n",
    "    axes[1,0].hist(DIAG['uncertainty'].mean(axis=1), bins=50, alpha=0.7, color='purple')\n",
    "    axes[1,0].set_title('Uncertainty Distribution'); axes[1,0].set_xlabel('Mean Variance')\n\n",
    "# 5. Diffusion trajectory\nif DIAG['diff_traj'] is not None and len(DIAG['diff_traj']) > 1:\n",
    "    norms = [np.linalg.norm(t[:10], axis=1).mean() for t in DIAG['diff_traj']]\n",
    "    axes[1,1].plot(norms, 'o-', color='green'); axes[1,1].set_title('Diffusion Trajectory (Norm)')\n",
    "    axes[1,1].set_xlabel('Step'); axes[1,1].set_ylabel('Mean Norm')\n\n",
    "# 6. Coverage bar\ncov = abl.get('coverage@50', 0)\nif isinstance(cov, dict): cov = list(cov.values())[0] if cov else 0\naxes[1,2].bar(['Coverage@50'], [cov], color='#3498db'); axes[1,2].set_ylim(0, 1); axes[1,2].set_title('Catalog Coverage')\n\n",
    "plt.tight_layout(); plt.savefig('/content/diagnostics.png', dpi=150); plt.show()\nprint('\u2713 Saved diagnostics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 11. Paper Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER = {'ItemKNNCF': 0.1175, 'CLCRec': 0.0815, 'EQUAL': 0.1310, 'NFC': 0.1904, 'MARec': 0.2928}\n\n",
    "our_best = max([abl[m].get('hr@10', 0) for m in abl if isinstance(abl[m], dict)], default=0)\n\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\nnames = list(PAPER.keys()) + ['Ours']; vals = list(PAPER.values()) + [our_best]\n",
    "colors = ['#95a5a6']*len(PAPER) + ['#e74c3c']\nax.barh(names, vals, color=colors)\n",
    "for i, v in enumerate(vals): ax.text(v+0.01, i, f'{v:.3f}', va='center')\n",
    "ax.set_xlabel('HR@10'); ax.set_title('Comparison with Paper (Table 3)')\n",
    "plt.tight_layout(); plt.savefig('/content/paper_comparison.png', dpi=150); plt.show()\nprint(f'Our best HR@10: {our_best:.4f} vs MARec paper: 0.2928')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\nmodels = [m for m in ['Baseline', '+CA', '+CA+UA', '+CA+UA+GE', '+CA+UA+GE+Diff'] if m in abl]\n",
    "if len(models) >= 2:\n    data = [[abl[m].get(f'hr@{k}', 0) for k in CONFIG['ks']] + [abl[m].get(f'ndcg@{k}', 0) for k in CONFIG['ks']] for m in models]\n",
    "    cols = [f'HR@{k}' for k in CONFIG['ks']] + [f'NDCG@{k}' for k in CONFIG['ks']]\n",
    "    df = pd.DataFrame(data, index=models, columns=cols)\n",
    "    plt.figure(figsize=(10, 4)); sns.heatmap(df, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "    plt.title('Ablation Heatmap'); plt.tight_layout(); plt.savefig('/content/ablation_heatmap.png', dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 12. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json as jm\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "rows = [{'model': m, **{k: round(v, 6) for k, v in r.items()}} for m, r in abl.items() if isinstance(r, dict)]\n",
    "pd.DataFrame(rows).to_csv(os.path.join(CONFIG['output_dir'], 'results.csv'), index=False)\n",
    "with open(os.path.join(CONFIG['output_dir'], 'config.json'), 'w') as f: jm.dump({k: str(v) for k, v in CONFIG.items()}, f, indent=2)\n",
    "for fn in ['diagnostics.png', 'paper_comparison.png', 'ablation_heatmap.png']:\n    if os.path.exists(f'/content/{fn}'): shutil.copy(f'/content/{fn}', CONFIG['output_dir'])\n",
    "shutil.make_archive(CONFIG['output_dir'], 'zip', CONFIG['output_dir'])\n",
    "total = time.time() - _NOTEBOOK_START\nprint(f'\\n\u2713 Results: {CONFIG[\"output_dir\"]}.zip')\nprint(f'\u2713 Total: {total:.0f}s ({total/60:.1f} min)')"
   ]
  }
 ]
}