{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import fsspec\n",
    "import math\n",
    "import datetime\n",
    "import random\n",
    "import bottleneck as bn\n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Embedding, Flatten, Dot, MultiHeadAttention, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1L2, l1, l2\n",
    "from tensorflow.keras.callbacks import Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8237b3",
   "metadata": {},
   "source": [
    "## Amazon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de227e",
   "metadata": {},
   "source": [
    "Amazon Video Games (Ni et al. 2019): it is a collection of reviews gathered from\n",
    "the Amazon website in a 22 years period between 1996 and 2018. Reviews contain ratings on a 5-star scale, and products are accompanied with detailed meta-\n",
    "data. Also in this case the preferences have been binarized as described for Mov-\n",
    "ielens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f0d11",
   "metadata": {},
   "source": [
    "### Now dealing with 80-20 cold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd480b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get click data \n",
    "path = \"data/splits/AmazonReviewData/AmazonGames/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/ICM_all.npz\"\n",
    "df_icm = sparse.load_npz(<path>)\n",
    "\n",
    "path = \"data/splits/AmazonReviewData/AmazonGames/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/cold_items_holdout_0.80_0.00_0.20_testtreshold_0.0_no_cold_users/URM_all_0_train.npz\"\n",
    "df_urm = sparse.load_npz(<path>)\n",
    "\n",
    "path = \"data/splits/AmazonReviewData/AmazonGames/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/cold_items_holdout_0.80_0.00_0.20_testtreshold_0.0_no_cold_users/URM_all_0_test.npz\"\n",
    "df_urm_test = sparse.load_npz(<path>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/splits/AmazonReviewData/AmazonGames/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/URM_all_mapper\"\n",
    "urm_mapper = pickle.loads(<path>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"data/splits/AmazonReviewData/AmazonGames/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/ICM_all_mapper\"\n",
    "icm_mapper = pickle.loads(<path>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e60f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"data/raw-datasets/AmazonReviewData/AmazonGames/Video_Games.json.gz\"\n",
    "#get json buffer\n",
    "buffer = BytesIO(response['Body'].read())\n",
    "with gzip.open(buffer, 'r') as fin:       \n",
    "    json_bytes = fin.read()   \n",
    "json_string = json_bytes.decode('utf-8')\n",
    "json_lines = json_string.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review data about asins (in the case of cold start, no data)\n",
    "metadata_big = {}\n",
    "for line in json_lines: \n",
    "    data = json.loads(line)\n",
    "    metadata_big[data[\"asin\"]]=data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata about asins\n",
    "metadata = {}\n",
    "for line in json_lines: \n",
    "    data = json.loads(line)\n",
    "    metadata[data[\"asin\"]]=data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = ['category','main_cat', 'brand', 'description','title', 'feature',  'date', 'price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b73cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_user_index = urm_mapper[0]\n",
    "dict_asin_index = icm_mapper[0]\n",
    "dict_index_asin = {}\n",
    "for key in dict_asin_index:\n",
    "    dict_index_asin[dict_asin_index[key]]=key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80937901",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = ['category','main_cat', 'brand', 'title', 'description', 'feature']\n",
    "\n",
    "list_category = []\n",
    "list_main_cat = []\n",
    "list_brand = []\n",
    "list_date = []\n",
    "list_price = []\n",
    "list_title = []\n",
    "list_description = []\n",
    "list_feature = []\n",
    "\n",
    "for key in dict_index_asin:\n",
    "    asin = dict_index_asin[key]\n",
    "    \n",
    "    #category\n",
    "    category = metadata[asin][\"category\"]\n",
    "    if '</span></span></span>' in category:\n",
    "        category.remove('</span></span></span>')\n",
    "    list_category.append(category)\n",
    "    \n",
    "    #main_cat\n",
    "    main_cat = metadata[asin][\"main_cat\"]\n",
    "    if main_cat!=\"\":\n",
    "        list_main_cat.append([main_cat])\n",
    "    else:\n",
    "        list_main_cat.append([])\n",
    "    \n",
    "    #brand\n",
    "    brand = metadata[asin][\"brand\"]\n",
    "    if brand !=\"\":\n",
    "        brand = brand.replace('by',\"\")\n",
    "        brand = brand.strip()\n",
    "        list_brand.append([brand])\n",
    "    else:\n",
    "        list_brand.append([])\n",
    "    \n",
    "    \n",
    "    #title\n",
    "    list_title.append(metadata[asin][\"title\"])\n",
    "    \n",
    "    #description\n",
    "    description = metadata[asin][\"description\"]\n",
    "    if description!=[]:\n",
    "        list_description.append(description[0])\n",
    "    else:\n",
    "        list_description.append(\"\")\n",
    "    \n",
    "    #feature\n",
    "    features = metadata[asin][\"feature\"]\n",
    "    features = ' '.join(features)\n",
    "    list_feature.append(features)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_list_category = []\n",
    "for temp_list in list_category:\n",
    "    total_list_category.extend(temp_list)\n",
    "\n",
    "total_list_main_cat = []\n",
    "for temp_list in list_main_cat:\n",
    "    total_list_main_cat.extend(temp_list)\n",
    "    \n",
    "total_list_brand = []\n",
    "for temp_list in list_brand:\n",
    "    total_list_brand.extend(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "pipe_title = Pipeline([('count', CountVectorizer(stop_words='english',max_features=max_features)),('tfid', TfidfTransformer())]).fit(list_title)\n",
    "data_title = pipe_title.transform(list_title).toarray()\n",
    "\n",
    "max_features = 2000\n",
    "pipe_feature = Pipeline([('count', CountVectorizer(stop_words='english',max_features=max_features)),('tfid', TfidfTransformer())]).fit(list_feature)\n",
    "data_feature = pipe_feature.transform(list_feature).toarray()\n",
    "\n",
    "max_features = 2000\n",
    "pipe_description = Pipeline([('count', CountVectorizer(stop_words='english',max_features=max_features)),('tfid', TfidfTransformer())]).fit(list_description)\n",
    "data_description = pipe_description.transform(list_description).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaaa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_brand = Counter(total_list_brand)\n",
    "df_brand = pd.DataFrame(list(counter_brand.items()),columns = ['brand','count'])\n",
    "df_brand = df_brand.sort_values(\"count\",ascending=False)\n",
    "df_brand= df_brand[df_brand[\"count\"]>=2]\n",
    "\n",
    "counter_category = Counter(total_list_category)\n",
    "df_category = pd.DataFrame(list(counter_category.items()),columns = ['category','count'])\n",
    "df_category = df_category.sort_values(\"count\",ascending=False)\n",
    "df_category = df_category[df_category[\"count\"]>=2]\n",
    "\n",
    "counter_main_cat = Counter(total_list_main_cat)\n",
    "df_main_cat = pd.DataFrame(list(counter_main_cat.items()),columns = ['main_cat','count'])\n",
    "df_main_cat = df_main_cat.sort_values(\"count\",ascending=False)\n",
    "df_main_cat = df_main_cat[df_main_cat[\"count\"]>=2]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_brand.brand))\n",
    "encoding_brand = mlb.fit_transform(list_brand)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_main_cat.main_cat))\n",
    "encoding_main_cat = mlb.fit_transform(list_main_cat)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_category.category))\n",
    "encoding_category = mlb.fit_transform(list_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_description.shape)\n",
    "print(data_title.shape)\n",
    "print(data_feature.shape)\n",
    "print(encoding_main_cat.shape)\n",
    "print(encoding_category.shape)\n",
    "print(encoding_brand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "Bdescription = scaler.fit_transform(data_description)\n",
    "Btitle = scaler.fit_transform(data_title)\n",
    "Bfeature = scaler.fit_transform(data_feature)\n",
    "Bmaincat = scaler.fit_transform(encoding_main_cat)\n",
    "Bcategory = scaler.fit_transform(encoding_category)\n",
    "Bbrand = scaler.fit_transform(encoding_brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34854c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bconstant = np.concatenate((Btitle, Bdescription,Bfeature, Bmaincat, Bcategory, Bbrand), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79377dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### item KNN\n",
    "def cosine_sim(encoding,shrinkage=1):\n",
    "    sim1 = encoding.dot(encoding.T)\n",
    "    norm_fi = np.linalg.norm(encoding,axis=1)\n",
    "    sim2 = np.outer(norm_fi,norm_fi)+shrinkage\n",
    "    sim = sim1/sim2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "#     tp1 = np.ones(len(tp))\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    \n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "#     tp1 =  1. / np.log2(2*np.ones(min(heldout_batch.getnnz(axis=1)),k))\n",
    "#     IDCG = np.array([tp1.sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    \n",
    "#     indexes = X_true_binary.sum(axis=1).nonzero()\n",
    "#     X_pred_binary = X_pred_binary[indexes]\n",
    "#     X_true_binary = X_true_binary[indexes]\n",
    "\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float32)\n",
    "#     recall = tmp / X_true_binary.sum(axis=1)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c53eb12",
   "metadata": {},
   "source": [
    "### Cosine similarity weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1483dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bf = cosine_sim(Bconstant, shrinkage=50)\n",
    "X= df_urm.toarray()\n",
    "Xf = X.dot(Bf)# that is the Xtilde\n",
    "size = Bf.shape[0]\n",
    "for i in range(size):\n",
    "    Bf[i,i]=0\n",
    "    \n",
    "X = np.asarray(X).reshape((X.shape[0]*X.shape[1],1))\n",
    "Xtot = np.asarray(Xf).reshape((Xf.shape[0]*Xf.shape[1],1))\n",
    "\n",
    "my_array = X.copy()\n",
    "my_array[my_array == 0] = 0.01\n",
    "my_array = my_array.flatten()\n",
    "\n",
    "reg = LinearRegression().fit(Xtot, X, sample_weight=my_array)\n",
    "\n",
    "pred_val = reg.coef_[0][0]*Xf#normalised Xtilde\n",
    "values = df_urm.nonzero()\n",
    "pred_val[values] =  -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8941505",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_urm_test.copy()\n",
    "\n",
    "n10_list, r10_list, n25_list, r25_list, n50_list, r50_list, n100_list, r100_list = [], [],[], [],[], [],[], []\n",
    "r10_list.append(Recall_at_k_batch(pred_val,test_data, k=10))\n",
    "n10_list.append(NDCG_binary_at_k_batch(pred_val,test_data, k=10))\n",
    "r25_list.append(Recall_at_k_batch(pred_val, test_data, k=25))\n",
    "n25_list.append(NDCG_binary_at_k_batch(pred_val, test_data, k=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 epochs filter at 3 streams, dropout 0.2, siamese with 2 layers (fraternal on 1st) and summed transformers outputs \n",
    "print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n25_list), np.nanstd(n25_list) / np.sqrt(len(n25_list))))\n",
    "print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r25_list), np.nanstd(r25_list) / np.sqrt(len(r25_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639902a4",
   "metadata": {},
   "source": [
    "### Test hybrid method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9764ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization\n",
    "\n",
    "X= df_urm.toarray()\n",
    "\n",
    "lambda1s = [1,500,1000]\n",
    "lambda2s = [1,10,100,1000]\n",
    "lambda3s = [1,10]\n",
    "coefs = [0.0]\n",
    "ks = [0]\n",
    "\n",
    "b = np.average(X,0)\n",
    "xo = np.mean(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_test_data = df_urm_vad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be583210",
   "metadata": {},
   "outputs": [],
   "source": [
    "n10_list, r10_list, n25_list, r25_list, n50_list, r50_list, n100_list, r100_list = [], [],[], [],[], [],[], []\n",
    "\n",
    "k= 0\n",
    "for coef in coefs:\n",
    "    for lambda1 in lambda1s:\n",
    "        for lambda2 in lambda2s:\n",
    "            for lambda3 in lambda3s:\n",
    "\n",
    "                perc_value = 10\n",
    "                rho = 1 \n",
    "                yo = lambda1\n",
    "                y = [lambda1+k*bi-k*xo for bi in b]\n",
    "\n",
    "\n",
    "                #transform vector to diag matrix IR\n",
    "                vector = np.sum(X, axis=0)\n",
    "                percentile = max(np.percentile(vector,perc_value),1)\n",
    "                k = lambda2/percentile\n",
    "                vector_tr = np.zeros(len(vector))\n",
    "                for counter,item in enumerate(vector):\n",
    "                    if item <= percentile:\n",
    "                        vector_tr[counter] = k*(percentile-item)\n",
    "\n",
    "                IR = np.diag(vector_tr)\n",
    "\n",
    "                #dense computations\n",
    "                #Xtilde\n",
    "#                 Xtilde = X.dot(B)\n",
    "                Xtilde= reg.coef_[0][0]*X.dot(Bf)#X1bis+X2bis+X3bis+X4bis+X5bis+X6bis#+X9bis#+lambdas[6][0]*X7+lambdas[7][0]*X8\n",
    "                print(\"Xtilde computed\")\n",
    "\n",
    "                #compute P\n",
    "                P0 = rho*X.T.dot(X)+coef*Bconstant.dot(Bconstant.T)+y*np.diag(np.ones(df_urm.shape[1]))+X.T.dot(Xtilde).dot(IR)\n",
    "                P = np.linalg.inv(P0)\n",
    "                print(\"P computed\")\n",
    "                del P0\n",
    "\n",
    "                #update of Bk\n",
    "                B_temp = rho*X.T.dot(X)+coef*Bconstant.dot(Bconstant.T)+lambda3*X.T.dot(Xtilde).dot(IR)\n",
    "                B_tilde = P.dot(B_temp)\n",
    "                gamma = np.diag(B_tilde) / np.diag(P)\n",
    "                B_k = B_tilde - P.dot(np.diag(gamma))\n",
    "                B_k = np.asarray(B_k).reshape((B_k.shape[0],B_k.shape[1]))\n",
    "                # del B_temp\n",
    "                # del B_tilde\n",
    "                # del gamma\n",
    "                print(\"B_k updated\")\n",
    "\n",
    "                val_index_min = 0\n",
    "                n10_list, n25_list, n50_list, n100_list, r10_list, r25_list, r50_list, r100_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "                Xtest = X\n",
    "\n",
    "                pred_val = Xtest.dot(B_k)\n",
    "                pred_val[values] = -np.inf\n",
    "\n",
    "\n",
    "                n10_list.append(NDCG_binary_at_k_batch(pred_val, vad_test_data, k=10))\n",
    "                n25_list.append(NDCG_binary_at_k_batch(pred_val, vad_test_data, k=25))\n",
    "                r10_list.append(Recall_at_k_batch(pred_val, vad_test_data, k=10))\n",
    "                r25_list.append(Recall_at_k_batch(pred_val, vad_test_data, k=25))\n",
    "\n",
    "\n",
    "                n10_list = np.concatenate(n10_list)\n",
    "                n25_list = np.concatenate(n25_list)\n",
    "                r10_list = np.concatenate(r10_list)\n",
    "                r25_list = np.concatenate(r25_list)\n",
    "                \n",
    "                #0cold-\n",
    "                print(\"lambda1={}\".format(str(lambda1)))\n",
    "                print(\"lambda2={}\".format(str(lambda2)))\n",
    "                print(\"lambda3={}\".format(str(lambda3)))\n",
    "                \n",
    "                print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "                print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "                \n",
    "                print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n25_list), np.nanstd(n25_list) / np.sqrt(len(n25_list))))\n",
    "                print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r25_list), np.nanstd(r25_list) / np.sqrt(len(r25_list))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
