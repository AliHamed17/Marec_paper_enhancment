{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy import sparse\n",
    "\n",
    "import bottleneck as bn\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Embedding, Flatten, Dot, MultiHeadAttention, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1L2, l1, l2\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe7dcc",
   "metadata": {},
   "source": [
    "## Loading training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.loads(\"<path>-train.pickle\")\n",
    "cold_test_data = pickle.loads(\"<path>-test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3409d64",
   "metadata": {},
   "source": [
    "## Get meta data and pre-computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(encoding,shrinkage=0.1):\n",
    "    sim1 = encoding.dot(encoding.T)\n",
    "    norm_fi = np.linalg.norm(encoding,axis=1)\n",
    "    sim2 = np.outer(norm_fi,norm_fi)+shrinkage\n",
    "    sim = sim1/sim2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content = json.loads(\"<path>-ML-10M-metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55164e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_actors = []\n",
    "list_genres = []\n",
    "list_directors = []\n",
    "vocabulary_title = []\n",
    "vocabulary_description = []\n",
    "list_tags = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb00f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create the IxI similarity matrices with the same indices the UxI matrix is constructed \n",
    "B6_0_id = []\n",
    "counter = 0\n",
    "# create similarity matrices now, same indexing as sparse UxI click matrix \n",
    "for key in json_content: \n",
    "    \n",
    "    data = json_content[key]['title'] \n",
    "    vocabulary_title.extend(data.split())\n",
    "    data = json_content[key]['description']\n",
    "    vocabulary_description.extend(data.split())\n",
    "    list_actors.extend(json_content[key]['actors'])\n",
    "    list_directors.extend(json_content[key]['directors']) \n",
    "    list_genres.extend(json_content[key]['genres'])\n",
    "    list_tags.extend(json_content[key]['tags'])\n",
    "    if json_content[key]['tags']==['Not']:\n",
    "        B6_0_id.append(counter)\n",
    "    counter += 1\n",
    "\n",
    "set_actors = set(list_actors)\n",
    "set_directors = set(list_directors)\n",
    "set_genres = set(list_genres)\n",
    "set_vocabulary_title = set(vocabulary_title)\n",
    "set_vocabulary_description = set(vocabulary_description)\n",
    "set_tags = set(list_tags)\n",
    "\n",
    "corpus_title = [json_content[key]['title'] for key in json_content]\n",
    "corpus_description = [json_content[key]['description'] for key in json_content]\n",
    "corpus_actors = [json_content[key]['actors'] for key in json_content]\n",
    "corpus_directors = [json_content[key]['directors'] for key in json_content]\n",
    "corpus_genres = [json_content[key]['genres'] for key in json_content]\n",
    "corpus_tags = [json_content[key]['tags'] for key in json_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "encoding_genres = mlb.fit_transform(corpus_genres)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoding_tags = mlb.fit_transform(corpus_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cdc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Falcon embeddings for concatenated title and description\n",
    "cat_llm_emb = pickle.loads(\"<path>-cat_embs.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bgenres = encoding_genres\n",
    "Bllmcat = cat_llm_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Bgenres = scaler.fit_transform(Bgenres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bactors = pickle.loads(\"<path>-actors_list.pickle\")\n",
    "Bdirs = pickle.loads(\"<path>-dirs_list.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bactors = np.array(Bactors)\n",
    "Bdirs = np.array(Bdirs)\n",
    "\n",
    "print(Bactors.shape)\n",
    "print(Bdirs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7c1fa",
   "metadata": {},
   "source": [
    "## Metrics defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    \n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "\n",
    "    return DCG / IDCG\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780c7eb",
   "metadata": {},
   "source": [
    "## Preparing training dataset for Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60209805",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bconstant = np.concatenate((Bllmcat, Bactors, Bdirs, Bgenres), axis=1)\n",
    "X= train_data.toarray()\n",
    "X= train_data.toarray()\n",
    "Bconstant_updated = Bconstant\n",
    "\n",
    "Xsum = np.sum(X,0)\n",
    "indices_ite = np.where(Xsum>=1)#at least 1 ratings per item\n",
    "X = X[:,indices_ite[0]]\n",
    "Bconstant_updated = Bconstant_updated[indices_ite[0],:]\n",
    "\n",
    "print(X.shape)\n",
    "print(Bconstant.shape)\n",
    "print(Bconstant_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06677b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_des_index = Bllmcat.shape[1]\n",
    "actor_index = title_des_index + Bactors.shape[1]\n",
    "dir_index = actor_index + Bdirs.shape[1]\n",
    "\n",
    "print(title_des_index)\n",
    "print(actor_index)\n",
    "print(dir_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bnew = cosine_similarity(X.T, Y=None, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07038a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INDICES = 50\n",
    "\n",
    "pos_samples = []\n",
    "neg_samples = []\n",
    "pos_couples = {}\n",
    "for i in range(len(Bnew)):\n",
    "    pos_couples[i]=[]\n",
    "\n",
    "for i in range(len(Bnew)):\n",
    "    \n",
    "    #argsort then sort \n",
    "    mylist = Bnew[i,:]\n",
    "    pos_indices = list(np.argsort(-mylist))\n",
    "\n",
    "    pos_indices.pop(0)\n",
    "    pos_indices = pos_indices[:MAX_INDICES]\n",
    "    \n",
    "    datafiltneg = list(np.where(Bnew[i,:]==0.)[0])\n",
    "    random.shuffle(datafiltneg)\n",
    "\n",
    "    counter_pos = 0\n",
    "    for indice in pos_indices:\n",
    "        if indice not in pos_couples:\n",
    "            pos_samples.append((i,indice,1))\n",
    "            pos_couples[i].append(indice)\n",
    "            counter_pos+=1\n",
    "        elif i not in pos_couples[indice]:\n",
    "            pos_samples.append((i,indice,1))\n",
    "            pos_couples[i].append(indice)\n",
    "            counter_pos+=1\n",
    "            \n",
    "    neg_indices = datafiltneg[:counter_pos]\n",
    "    for indice in neg_indices:\n",
    "        neg_samples.append((i,indice,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_samples_train = random.sample(pos_samples, k=round(len(pos_samples) * 0.8))\n",
    "pos_samples_test = list(set(pos_samples_train) ^ set(pos_samples))\n",
    "\n",
    "neg_samples_train = random.sample(neg_samples, k=round(len(neg_samples) * 0.8))\n",
    "neg_samples_test = list(set(neg_samples_train) ^ set(neg_samples))\n",
    "\n",
    "samples_train = []\n",
    "samples_train.extend(pos_samples_train)\n",
    "samples_train.extend(neg_samples_train)\n",
    "\n",
    "samples_test = []\n",
    "samples_test.extend(pos_samples_test)\n",
    "samples_test.extend(neg_samples_test)\n",
    "\n",
    "samples = []\n",
    "samples.extend(pos_samples)\n",
    "samples.extend(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe075912",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(samples_train)\n",
    "random.shuffle(samples_test)\n",
    "random.shuffle(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4709a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, B, samples,\n",
    "                 batch_size,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.B = B\n",
    "        self.samples = samples\n",
    "        self.shuffle = shuffle  \n",
    "        self.n = len(self.samples)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.samples)\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #lets start simple: batch is multiple of num_items\n",
    "        \n",
    "        batched_samples = self.samples[index * self.batch_size:(index+1) * self.batch_size]\n",
    "        indexes_i = [item[0] for item in batched_samples]\n",
    "        indexes_j = [item[1] for item in batched_samples]\n",
    "        y = np.array([item[2] for item in batched_samples])\n",
    "        batches = (np.array(self.B[indexes_i,:]),np.array(self.B[indexes_j,:]))    \n",
    "        X_batch = (batches, y)   \n",
    "        \n",
    "                \n",
    "        return X_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f75094",
   "metadata": {},
   "source": [
    "## Defining Cross-Attention Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF model\n",
    "embedding_dim = 768 // 2\n",
    "num_heads = 3\n",
    "\n",
    "input1 = Input(shape=(Bconstant_updated.shape[1],),name='ratings1')\n",
    "input2 = Input(shape=(Bconstant_updated.shape[1],),name='ratings2')\n",
    "\n",
    "# FCs [proj, i] project sparse features only (all together)\n",
    "layeract = Dense(768, activation=\"linear\",name='layer1')\n",
    "layermulti = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "\n",
    "T11 = layeract(input1[:,dir_index:]) # all categorical\n",
    "T21 = layeract(input2[:,dir_index:])# all categorical\n",
    "\n",
    "T12 = tf.expand_dims(Dense(embedding_dim)(input1[:,:title_des_index]), axis=1) # concatnated title/description\n",
    "T13 = tf.expand_dims(Dense(embedding_dim)(input1[:,title_des_index:actor_index]), axis=1) # actors\n",
    "T14 = tf.expand_dims(Dense(embedding_dim)(input1[:,actor_index:dir_index]), axis=1) # directors\n",
    "\n",
    "T22 = tf.expand_dims(Dense(embedding_dim)(input2[:,:title_des_index]), axis=1) # concatnated title/description\n",
    "T23 = tf.expand_dims(Dense(embedding_dim)(input2[:,title_des_index:actor_index]), axis=1) # actors\n",
    "T24 = tf.expand_dims(Dense(embedding_dim)(input2[:,actor_index:dir_index]), axis=1) # directors\n",
    "\n",
    "T1X = tf.squeeze(layermulti(layermulti(T12, T13), T14), axis=1) # CROSS 1x\n",
    "T2X = tf.squeeze(layermulti(layermulti(T22, T23), T24), axis=1) # CROSS 1x\n",
    "\n",
    "T31 = Concatenate(axis=1)([T1X, T11])\n",
    "T32 = Concatenate(axis=1)([T2X, T21])\n",
    "\n",
    "layer3 = Dense(768, activation=\"relu\",name='layer3')\n",
    "layer4 = Dense(768, activation=\"relu\",name='layer4')\n",
    "layerattention = Attention(use_scale=True,name='att')\n",
    "\n",
    "T31 = layer4(layer3(T31))\n",
    "T32 = layer4(layer3(T32))\n",
    "\n",
    "# scaled pairwise cosine similarities (+shrinkage?) [i, i]\n",
    "T3 = Dot(axes=(1, 1), normalize=True)([T31, T32])\n",
    "\n",
    "# # our model will accept the inputs of the two branches and then output a single value\n",
    "model = Model(inputs=[input1,input2], outputs=T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9604b7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67080b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,label_smoothing=0.)\n",
    "optimizer = tf.keras.optimizers.experimental.Nadam(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "traingen = CustomDataGen(B=Bconstant_updated, samples=samples_train, batch_size=256)\n",
    "vadgen = CustomDataGen(B=Bconstant_updated, samples=samples_test, batch_size=256)\n",
    "finaltraingen = CustomDataGen(B=Bconstant_updated, samples=samples, batch_size=256)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "# model.fit(traingen,epochs=num_epochs,validation_data=vadgen)\n",
    "model.fit(finaltraingen,epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2f6da",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_network = Model(model.input, model.get_layer('layer3').output)\n",
    "feature = feature_network.predict([Bconstant,Bconstant])\n",
    "B =  cosine_similarity(feature, dense_output=True)\n",
    "np.fill_diagonal(B,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X= train_data.toarray()\n",
    "Bf = B.copy()\n",
    "Xf = X.dot(Bf)\n",
    "size = Bf.shape[0]\n",
    "for i in range(size):\n",
    "    Bf[i,i]=0\n",
    "X = np.asarray(X).reshape((X.shape[0]*X.shape[1],1))\n",
    "Xtot = np.asarray(Xf).reshape((Xf.shape[0]*Xf.shape[1],1))\n",
    "my_array = X.copy()\n",
    "my_array[my_array == 0] = 0.01\n",
    "my_array = my_array.flatten()\n",
    "reg = LinearRegression().fit(Xtot, X, sample_weight=my_array)\n",
    "pred_val = reg.coef_[0][0]*Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA-Siamese on cold\n",
    "X = train_data.toarray()\n",
    "pred_val[X.nonzero()] = -np.inf\n",
    "n10_list, r10_list, n20_list, r20_list, n50_list, r50_list, n100_list, r100_list = [], [],[], [],[], [],[], []\n",
    "r10_list.append(Recall_at_k_batch(pred_val,cold_test_data, k=10))\n",
    "n10_list.append(NDCG_binary_at_k_batch(pred_val,cold_test_data, k=10))\n",
    "r20_list.append(Recall_at_k_batch(pred_val, cold_test_data, k=20))\n",
    "n20_list.append(NDCG_binary_at_k_batch(pred_val, cold_test_data, k=20))\n",
    "r50_list.append(Recall_at_k_batch(pred_val, cold_test_data, k=50))\n",
    "n50_list.append(NDCG_binary_at_k_batch(pred_val, cold_test_data, k=50))\n",
    "r100_list.append(Recall_at_k_batch(pred_val, cold_test_data, k=100))\n",
    "n100_list.append(NDCG_binary_at_k_batch(pred_val, cold_test_data, k=100))\n",
    "\n",
    "print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "print(\"Test NDCG@20=%.5f (%.5f)\" % (np.nanmean(n20_list), np.nanstd(n20_list) / np.sqrt(len(n20_list))))\n",
    "print(\"Test NDCG@50=%.5f (%.5f)\" % (np.nanmean(n50_list), np.nanstd(n50_list) / np.sqrt(len(n50_list))))\n",
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.nanmean(n100_list), np.nanstd(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.nanmean(r20_list), np.nanstd(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.nanmean(r50_list), np.nanstd(r50_list) / np.sqrt(len(r50_list))))\n",
    "print(\"Test Recall@100=%.5f (%.5f)\" % (np.nanmean(r100_list), np.nanstd(r100_list) / np.sqrt(len(r100_list))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
