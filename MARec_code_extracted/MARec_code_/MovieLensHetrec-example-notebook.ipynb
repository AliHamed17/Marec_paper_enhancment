{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31708986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import fsspec\n",
    "import math\n",
    "import random\n",
    "import bottleneck as bn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import sparse\n",
    "from collections import Countershrinkage = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f22b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Embedding, Flatten, Dot#, MultiHeadAttention, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1L2, l1, l2\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8fad6",
   "metadata": {},
   "source": [
    "## MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92de1b5",
   "metadata": {},
   "source": [
    "#### Movielens Hetrec: \n",
    "This dataset is an extension of MovieLens10M dataset and links the movies of MovieLens dataset with their corresponding web pages at Internet Movie Database (IMDb) and Rotten Tomatoes movie review systems. As item features, we kept only movie years, genres, actors, directors, countries, and locations. The preferences have been binarized considering all the ratings greater or equal to 3.5 as positive preferences.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6bf73",
   "metadata": {},
   "source": [
    "### All MovieLens Hetrec features processed in the NFC paper are treated categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e69bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1- movie tags\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_tags.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))\n",
    "# Create the pandas DataFrame\n",
    "df_movie_tags = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "#2- movies (year but also anything relevant)\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movies.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))\n",
    "# Create the pandas DataFrame\n",
    "df_movies = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "#3- movie genres\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_genres.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))\n",
    "# Create the pandas DataFrame\n",
    "df_movie_genres = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "#4- movie actors\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_actors.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))\n",
    "# Create the pandas DataFrame\n",
    "df_movie_actors = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "#5- movie directors\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_directors.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\")) \n",
    "# Create the pandas DataFrame\n",
    "df_movie_directors = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "path = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_countries.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))  \n",
    "# Create the pandas DataFrame\n",
    "df_movie_countries = pd.DataFrame(final_list[1:], columns = final_list[0])\n",
    "\n",
    "#5- movie locations\n",
    "key = \"data/raw-datasets/MovielensHetrec/hetrec2011-movielens-2k-v2/movie_locations.dat\"\n",
    "#write a script to load dt data: data = load(<path>)\n",
    "lines = data.split(\"\\n\")\n",
    "final_list = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"\\r\", \"\")\n",
    "    final_list.append(line.split(\"\\t\"))  \n",
    "# Create the pandas DataFrame\n",
    "df_movie_locations = pd.DataFrame(final_list[1:], columns = final_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get click data \n",
    "path = \"data/splits/MovielensHetrec/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/ICM_all.npz\"\n",
    "df_icm = sparse.load_npz(<path>)\n",
    "\n",
    "path = \"data/splits/MovielensHetrec/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/cold_items_holdout_0.80_0.00_0.20_testtreshold_0.0_no_cold_users/URM_all_0_train.npz\"\n",
    "df_urm = sparse.load_npz(<path>)\n",
    "\n",
    "key = \"kdd-data/splits/MovielensHetrec/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/cold_items_holdout_0.80_0.00_0.20_testtreshold_0.0_no_cold_users/URM_all_0_test.npz\"\n",
    "df_urm_test = sparse.load_npz(<path>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement read_pickle method\n",
    "urm_all = read_pickle(\"data/splits/MovielensHetrec/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/URM_all_mapper\")\n",
    "icm_all = read_pickle(\"data/splits/MovielensHetrec/original/implicit_3.0/kcore_user_5_item_5_5_feature_5_reshaped/ICM_all_mapper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e011df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urm_all is ratings matrix indexes, nb_users x number of movies \n",
    "dict_movie_indexes = urm_all[1] \n",
    "dict_user_indexes = urm_all[0]\n",
    "\n",
    "dict_index_movies = {}\n",
    "for key in dict_movie_indexes:\n",
    "    dict_index_movies[dict_movie_indexes[key]]=key\n",
    "\n",
    "movieIDs = df_movies.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare ratings and metadata matrices (train, vad and test data)\n",
    "\n",
    "#tags, years, genres, actors (no filtering for countries, directors)\n",
    "dict_tags = {}\n",
    "for i in range(len(df_movie_tags)):\n",
    "    ID = df_movie_tags.movieID.iloc[i]\n",
    "    item = df_movie_tags.tagID.iloc[i]\n",
    "    if not dict_tags.__contains__(ID):\n",
    "        dict_tags[ID] = []\n",
    "        dict_tags[ID].append(item)\n",
    "    else:\n",
    "        dict_tags[ID].append(item)\n",
    "\n",
    "dict_years = {}\n",
    "for i in range(len(df_movies)):\n",
    "    ID = df_movies.id.iloc[i]\n",
    "    item = df_movies.year.iloc[i]\n",
    "    if not dict_years.__contains__(ID):\n",
    "        dict_years[ID] = []\n",
    "        dict_years[ID].append(item)\n",
    "    else:\n",
    "        dict_years[ID].append(item)\n",
    "        \n",
    "dict_genres = {}\n",
    "for i in range(len(df_movie_genres)):\n",
    "    ID = df_movie_genres.movieID.iloc[i]\n",
    "    item = df_movie_genres.genre.iloc[i]\n",
    "    if not dict_genres.__contains__(ID):\n",
    "        dict_genres[ID] = []\n",
    "        dict_genres[ID].append(item)\n",
    "    else:\n",
    "        dict_genres[ID].append(item)\n",
    "    \n",
    "dict_actors = {}\n",
    "for i in range(len(df_movie_actors)):\n",
    "    ID = df_movie_actors.movieID.iloc[i]\n",
    "    item = df_movie_actors.actorName.iloc[i]\n",
    "    if not dict_actors.__contains__(ID):\n",
    "        dict_actors[ID] = []\n",
    "        dict_actors[ID].append(item)\n",
    "    else:\n",
    "        dict_actors[ID].append(item)\n",
    "        \n",
    "dict_directors = {}\n",
    "for i in range(len(df_movie_directors)):\n",
    "    ID = df_movie_directors.movieID.iloc[i]\n",
    "    item = df_movie_directors.directorName.iloc[i]\n",
    "    if not dict_directors.__contains__(ID):\n",
    "        dict_directors[ID] = []\n",
    "        dict_directors[ID].append(item)\n",
    "    else:\n",
    "        dict_directors[ID].append(item)\n",
    "        \n",
    "dict_countries = {}\n",
    "for i in range(len(df_movie_countries)):\n",
    "    ID = df_movie_countries.movieID.iloc[i]\n",
    "    item = df_movie_countries.country.iloc[i]\n",
    "    if not dict_countries.__contains__(ID):\n",
    "        dict_countries[ID] = []\n",
    "        dict_countries[ID].append(item)\n",
    "    else:\n",
    "        dict_countries[ID].append(item)\n",
    "        \n",
    "dict_location1={}\n",
    "dict_location2={}\n",
    "dict_location3={}\n",
    "\n",
    "for i in range(len(df_movie_locations)):\n",
    "    ID = df_movie_locations.movieID.iloc[i]\n",
    "    item1 = df_movie_locations.location1.iloc[i]\n",
    "    item2 = df_movie_locations.location2.iloc[i]\n",
    "    item3 = df_movie_locations.location3.iloc[i]\n",
    "    if not dict_location1.__contains__(ID):\n",
    "        dict_location1[ID] =[]\n",
    "        dict_location1[ID].append(item1)\n",
    "        dict_location2[ID] =[]\n",
    "        dict_location2[ID].append(item2)\n",
    "        dict_location3[ID] =[]\n",
    "        dict_location3[ID].append(item3)\n",
    "        \n",
    "    else:\n",
    "        if item1 not in dict_location1[ID]:\n",
    "            dict_location1[ID].append(item1)\n",
    "        if item2 not in dict_location2[ID]:\n",
    "            dict_location2[ID].append(item2)\n",
    "        if item3 not in dict_location3[ID]:\n",
    "            dict_location3[ID].append(item3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tags = []\n",
    "list_years = []\n",
    "list_genres = []\n",
    "list_actors = []\n",
    "list_directors = []\n",
    "list_countries = []\n",
    "list_location1 = []\n",
    "list_location2 = []\n",
    "list_location3 = []\n",
    "\n",
    "for key in dict_index_movies:\n",
    "    \n",
    "    #tags\n",
    "    if dict_tags.__contains__(dict_index_movies[key]):\n",
    "        list_tags.append(dict_tags[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_tags.append([])\n",
    "        \n",
    "    #years\n",
    "    if dict_years.__contains__(dict_index_movies[key]):\n",
    "        list_years.append([int(dict_years[dict_index_movies[key]][0])])\n",
    "    else:\n",
    "        list_years.append([])\n",
    "        \n",
    "    #genres\n",
    "    if dict_genres.__contains__(dict_index_movies[key]):\n",
    "        list_genres.append(dict_genres[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_genres.append([])\n",
    "        \n",
    "    #actors\n",
    "    if dict_actors.__contains__(dict_index_movies[key]):\n",
    "        list_actors.append(dict_actors[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_actors.append([])\n",
    "        \n",
    "    #directors\n",
    "    if dict_directors.__contains__(dict_index_movies[key]):\n",
    "        list_directors.append(dict_directors[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_directors.append([])\n",
    "        \n",
    "    #countries\n",
    "    if dict_countries.__contains__(dict_index_movies[key]):\n",
    "        list_countries.append(dict_countries[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_countries.append([])\n",
    "        \n",
    "    #location1\n",
    "    if dict_location1.__contains__(dict_index_movies[key]):\n",
    "        list_location1.append(dict_location1[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_location1.append([])\n",
    "        \n",
    "    #location2\n",
    "    if dict_location2.__contains__(dict_index_movies[key]):\n",
    "        list_location2.append(dict_location2[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_location2.append([])\n",
    "        \n",
    "    #location3\n",
    "    if dict_location3.__contains__(dict_index_movies[key]):\n",
    "        list_location3.append(dict_location3[dict_index_movies[key]])\n",
    "    else:\n",
    "        list_location3.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dceb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_list_actors = []\n",
    "for temp_list in list_actors:\n",
    "    total_list_actors.extend(temp_list)\n",
    "\n",
    "total_list_directors = []\n",
    "for temp_list in list_directors:\n",
    "    total_list_directors.extend(temp_list)\n",
    "    \n",
    "total_list_genres = []\n",
    "for temp_list in list_genres:\n",
    "    total_list_genres.extend(temp_list)\n",
    "    \n",
    "total_list_tags = []\n",
    "for temp_list in list_tags:\n",
    "    total_list_tags.extend(temp_list)\n",
    "    \n",
    "total_list_location1 = []\n",
    "for temp_list in list_location1:\n",
    "    total_list_location1.extend(temp_list)\n",
    "\n",
    "total_list_location2 = []\n",
    "for temp_list in list_location2:\n",
    "    total_list_location2.extend(temp_list)\n",
    "    \n",
    "total_list_location3 = []\n",
    "for temp_list in list_location3:\n",
    "    total_list_location3.extend(temp_list)\n",
    "    \n",
    "total_list_countries = []\n",
    "for temp_list in list_countries:\n",
    "    total_list_countries.extend(temp_list)\n",
    "    \n",
    "total_list_countries.remove('')\n",
    "total_list_location1.remove('')\n",
    "total_list_location2.remove('')\n",
    "total_list_location3.remove('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_locations = []\n",
    "for i in range(len(list_location1)):\n",
    "    list_locations.append(' '.join(list_location1[i])+' '+' '.join(list_location2[i])+' '+' '.join(list_location3[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_actors = Counter(total_list_actors)\n",
    "df_actors = pd.DataFrame(list(counter_actors.items()),columns = ['actors','count'])\n",
    "df_actors = df_actors.sort_values(\"count\",ascending=False)\n",
    "# max_features = len(df_actors)//5\n",
    "df_actors= df_actors[df_actors[\"count\"]>=2]\n",
    "# df_actors = df_actors.iloc[:max_features]\n",
    "\n",
    "counter_directors = Counter(total_list_directors)\n",
    "df_directors = pd.DataFrame(list(counter_directors.items()),columns = ['directors','count'])\n",
    "df_directors = df_directors.sort_values(\"count\",ascending=False)\n",
    "df_directors= df_directors[df_directors[\"count\"]>=2]\n",
    "\n",
    "counter_genres = Counter(total_list_genres)\n",
    "df_genres = pd.DataFrame(list(counter_genres.items()),columns = ['genres','count'])\n",
    "df_genres = df_genres.sort_values(\"count\",ascending=False)\n",
    "df_genres = df_genres[df_genres[\"count\"]>=2]\n",
    "\n",
    "counter_tags = Counter(total_list_tags)\n",
    "df_tags = pd.DataFrame(list(counter_tags.items()),columns = ['tags','count'])\n",
    "df_tags = df_tags.sort_values(\"count\",ascending=False)\n",
    "df_tags = df_tags[df_tags[\"count\"]>=2]\n",
    "\n",
    "counter_countries = Counter(total_list_countries)\n",
    "df_countries = pd.DataFrame(list(counter_countries.items()),columns = ['countries','count'])\n",
    "df_countries = df_countries.sort_values(\"count\",ascending=False)\n",
    "df_countries = df_countries[df_countries[\"count\"]>=2]\n",
    "\n",
    "counter_location1 = Counter(total_list_location1)\n",
    "df_location1 = pd.DataFrame(list(counter_location1.items()),columns = ['location1','count'])\n",
    "df_location1 = df_location1.sort_values(\"count\",ascending=False)\n",
    "df_location1 = df_location1[df_location1[\"count\"]>=2]\n",
    "\n",
    "counter_location2 = Counter(total_list_location2)\n",
    "df_location2 = pd.DataFrame(list(counter_location2.items()),columns = ['location2','count'])\n",
    "df_location2 = df_location2.sort_values(\"count\",ascending=False)\n",
    "df_location2 = df_location2[df_location2[\"count\"]>=2]\n",
    "\n",
    "counter_location3 = Counter(total_list_location3)\n",
    "df_location3 = pd.DataFrame(list(counter_location3.items()),columns = ['location3','count'])\n",
    "df_location3 = df_location3.sort_values(\"count\",ascending=False)\n",
    "df_location3 = df_location3[df_location3[\"count\"]>=2]\n",
    "\n",
    "max_features = 10000\n",
    "pipe_feature = Pipeline([('count', CountVectorizer(max_features=max_features)),('tfid', TfidfTransformer())]).fit(list_locations)\n",
    "encoding_locations = pipe_feature.transform(list_locations).toarray()\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_actors.actors))\n",
    "encoding_actors = mlb.fit_transform(list_actors)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_directors.directors))\n",
    "encoding_directors = mlb.fit_transform(list_directors)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_genres.genres))\n",
    "encoding_genres = mlb.fit_transform(list_genres)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_tags.tags))\n",
    "encoding_tags = mlb.fit_transform(list_tags)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_location1.location1))\n",
    "encoding_location1 = mlb.fit_transform(list_location1)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_location2.location2))\n",
    "encoding_location2 = mlb.fit_transform(list_location2)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_location3.location3))\n",
    "encoding_location3 = mlb.fit_transform(list_location3)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_countries.countries))\n",
    "encoding_countries = mlb.fit_transform(list_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bb3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text features\n",
    "print(encoding_tags.shape)\n",
    "\n",
    "#genre features\n",
    "print(encoding_genres.shape)\n",
    "\n",
    "#cast features\n",
    "print(encoding_actors.shape)\n",
    "print(encoding_directors.shape)\n",
    "\n",
    "#geographic features\n",
    "print(encoding_countries.shape)\n",
    "print(encoding_location1.shape)\n",
    "print(encoding_location2.shape)\n",
    "print(encoding_location3.shape)\n",
    "print(encoding_locations.shape)\n",
    "\n",
    "# + dont forget years that should not be part of an embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa554666",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "Btags = scaler.fit_transform(encoding_tags)\n",
    "Bgenres = scaler.fit_transform(encoding_genres)\n",
    "Bactors = scaler.fit_transform(encoding_actors)\n",
    "Bdirectors = scaler.fit_transform(encoding_directors)\n",
    "Bcountries = scaler.fit_transform(encoding_countries)\n",
    "Blocation1 = scaler.fit_transform(encoding_location1)\n",
    "Blocation2 = scaler.fit_transform(encoding_location2)\n",
    "Blocation3 = scaler.fit_transform(encoding_location3)\n",
    "Blocations = scaler.fit_transform(encoding_locations)\n",
    "Byears = scaler.fit_transform(list_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(encoding,shrinkage=0.1):\n",
    "    sim1 = encoding.dot(encoding.T)\n",
    "    norm_fi = np.linalg.norm(encoding,axis=1)\n",
    "    sim2 = np.outer(norm_fi,norm_fi)+shrinkage\n",
    "    sim = sim1/sim2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrinkage = 20\n",
    "\n",
    "# B1 = cosine_sim(Btags, shrinkage = shrinkage)\n",
    "B2 = cosine_sim(Bgenres, shrinkage = shrinkage)\n",
    "B3 = cosine_sim(Bactors, shrinkage = shrinkage)\n",
    "B4 = cosine_sim(Bdirectors, shrinkage = shrinkage)\n",
    "B5 = cosine_sim(Bcountries, shrinkage = shrinkage)\n",
    "B6 = cosine_sim(Blocation1, shrinkage = shrinkage)\n",
    "B7 = cosine_sim(Blocation2, shrinkage = shrinkage)\n",
    "B8 = cosine_sim(Blocation3, shrinkage = shrinkage)\n",
    "B9 = euclidean_distances(Byears)\n",
    "B10 = cosine_sim(Blocations, shrinkage = shrinkage)\n",
    "\n",
    "B9 = B9/B9.max()\n",
    "B9 = 1-B9\n",
    "Bf = cosine_sim(Bconstant,shrinkage =shrinkage)\n",
    "\n",
    "size = B2.shape[0]\n",
    "for i in range(size):\n",
    "#     B1[i,i]=0\n",
    "    B2[i,i]=0\n",
    "    B3[i,i]=0\n",
    "    B4[i,i]=0     \n",
    "    B5[i,i]=0\n",
    "    B6[i,i]=0\n",
    "    B7[i,i]=0\n",
    "    B8[i,i]=0\n",
    "    B9[i,i]=0\n",
    "    B10[i,i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604124c2",
   "metadata": {},
   "source": [
    "### 2- Cosine similarity weighting, linear weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0da09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_urm.toarray()\n",
    "X2 = X.dot(B2)\n",
    "X3 = X.dot(B3)\n",
    "X4 = X.dot(B4)\n",
    "X5 = X.dot(B5)\n",
    "X6 = X.dot(B6)\n",
    "X7 = X.dot(B7)\n",
    "X8 = X.dot(B8)\n",
    "X9 = X.dot(B9)\n",
    "X10 = X.dot(B10)\n",
    "\n",
    "X = np.asarray(X).reshape((X.shape[0]*X.shape[1],1))\n",
    "Xtot = np.concatenate((np.asarray(X2).reshape((X2.shape[0]*X2.shape[1],1)),\n",
    "                      np.asarray(X3).reshape((X3.shape[0]*X3.shape[1],1)),\n",
    "                      np.asarray(X4).reshape((X4.shape[0]*X4.shape[1],1)),\n",
    "                      np.asarray(X5).reshape((X5.shape[0]*X5.shape[1],1)),\n",
    "                      np.asarray(X6).reshape((X6.shape[0]*X6.shape[1],1)),\n",
    "                       np.asarray(X7).reshape((X7.shape[0]*X7.shape[1],1)),\n",
    "                       np.asarray(X8).reshape((X8.shape[0]*X8.shape[1],1)),\n",
    "                       np.asarray(X9).reshape((X9.shape[0]*X9.shape[1],1)),\n",
    "                      np.asarray(X10).reshape((X10.shape[0]*X10.shape[1],1))),axis=1)\n",
    "\n",
    "array_sum = np.sum(Xtot)\n",
    "array_has_nan = np.isnan(array_sum)\n",
    "\n",
    "print(Xtot.shape)\n",
    "\n",
    "\n",
    "my_array = X.copy()\n",
    "my_array[my_array == 0] = 0.2\n",
    "my_array = my_array.flatten()\n",
    "\n",
    "reg = LinearRegression().fit(Xtot, X, sample_weight=my_array)\n",
    "\n",
    "\n",
    "X2bis = reg.coef_[0][0]*X2\n",
    "X3bis = reg.coef_[0][1]*X3\n",
    "X4bis = reg.coef_[0][2]*X4\n",
    "X5bis = reg.coef_[0][3]*X5\n",
    "X6bis = reg.coef_[0][4]*X6\n",
    "X7bis = reg.coef_[0][5]*X7\n",
    "X8bis = reg.coef_[0][6]*X8\n",
    "X9bis = reg.coef_[0][7]*X9\n",
    "X10bis = reg.coef_[0][8]*X10\n",
    "                      \n",
    "pred_val = X2bis+X3bis+X4bis+X5bis+X6bis+X7bis+X8bis+X9bis+X10bis\n",
    "values = df_urm.nonzero()\n",
    "pred_val[values] =  -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dcc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    \n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_urm_test\n",
    "\n",
    "n10_list, r10_list, n25_list, r25_list, n50_list, r50_list, n100_list, r100_list = [], [],[], [],[], [],[], []\n",
    "r10_list.append(Recall_at_k_batch(pred_val,test_data, k=10))\n",
    "n10_list.append(NDCG_binary_at_k_batch(pred_val,test_data, k=10))\n",
    "r25_list.append(Recall_at_k_batch(pred_val, test_data, k=25))\n",
    "n25_list.append(NDCG_binary_at_k_batch(pred_val, test_data, k=25))\n",
    "r50_list.append(Recall_at_k_batch(pred_val, test_data, k=50))\n",
    "n50_list.append(NDCG_binary_at_k_batch(pred_val, test_data, k=50))\n",
    "r100_list.append(Recall_at_k_batch(pred_val, test_data, k=100))\n",
    "n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final metrics: shrinkage =20\n",
    "print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n25_list), np.nanstd(n25_list) / np.sqrt(len(n25_list))))\n",
    "print(\"Test NDCG@50=%.5f (%.5f)\" % (np.nanmean(n50_list), np.nanstd(n50_list) / np.sqrt(len(n50_list))))\n",
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.nanmean(n100_list), np.nanstd(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r25_list), np.nanstd(r25_list) / np.sqrt(len(r25_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.nanmean(r50_list), np.nanstd(r50_list) / np.sqrt(len(r50_list))))\n",
    "print(\"Test Recall@100=%.5f (%.5f)\" % (np.nanmean(r100_list), np.nanstd(r100_list) / np.sqrt(len(r100_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462cb0d",
   "metadata": {},
   "source": [
    "## MARec Hybrid solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb90f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_urm.toarray()\n",
    "values = df_urm.nonzero()\n",
    "\n",
    "\n",
    "#initialization\n",
    "lambda1s = [0.1,1,10,100,1000]\n",
    "lambda2s = [0.1,1,10,100,1000]\n",
    "lambda3s = [0.1,1,10,100]\n",
    "coefs = [0]\n",
    "\n",
    "ks = [0]\n",
    "\n",
    "b = np.average(X,0)\n",
    "xo = np.mean(b)\n",
    "\n",
    "vad_test_data =df_urm_vad\n",
    "\n",
    "for coef in coefs:\n",
    "    for k in ks:\n",
    "        for lambda1 in lambda1s:\n",
    "            for lambda2 in lambda2s:\n",
    "                for lambda3 in lambda3s:\n",
    "\n",
    "                    perc_value = 10\n",
    "                    rho = 1 \n",
    "                    yo = lambda1\n",
    "                    y = [lambda1+k*bi-k*xo for bi in b]\n",
    "\n",
    "\n",
    "                    #transform vector to diag matrix IR\n",
    "                    vector = np.sum(X, axis=0)\n",
    "                    percentile = max(np.percentile(vector,perc_value),1)\n",
    "                    k = lambda2/percentile\n",
    "                    vector_tr = np.zeros(len(vector))\n",
    "                    for counter,item in enumerate(vector):\n",
    "                        if item <= percentile:\n",
    "                            vector_tr[counter] = k*(percentile-item)\n",
    "\n",
    "                    IR = np.diag(vector_tr)\n",
    "\n",
    "                    #dense computations\n",
    "                    #Xtilde\n",
    "                    Xtilde = X2bis+X3bis+X4bis+X5bis+X6bis+X7bis+X8bis+X9bis+X10bis\n",
    "                    Xtilde = lambda3*Xtilde\n",
    "                    print(\"Xtilde computed\")\n",
    "\n",
    "                    #compute P\n",
    "                    P0 = rho*X.T.dot(X)+y*np.diag(np.ones(df_urm.shape[1]))+X.T.dot(Xtilde).dot(IR)\n",
    "                    P = np.linalg.inv(P0)\n",
    "                    print(\"P computed\")\n",
    "                    del P0\n",
    "\n",
    "                    #update of Bk\n",
    "                    B_temp = rho*X.T.dot(X)+X.T.dot(Xtilde).dot(IR)\n",
    "                    B_tilde = P.dot(B_temp)\n",
    "                    gamma = np.diag(B_tilde) / np.diag(P)\n",
    "                    B_k = B_tilde - P.dot(np.diag(gamma))\n",
    "                    B_k = np.asarray(B_k).reshape((B_k.shape[0],B_k.shape[1]))\n",
    "                    print(\"B_k updated\")\n",
    "\n",
    "                    val_index_min = 0\n",
    "                    n10_list, n25_list, n50_list, n100_list, r10_list, r25_list, r50_list, r100_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "                    Xtest = X\n",
    "\n",
    "                    pred_val = Xtest.dot(B_k)\n",
    "                    pred_val[values] = -np.inf\n",
    "\n",
    "                    # exclude examples from training and validation (if any)\n",
    "                    n10_list.append(NDCG_binary_at_k_batch(pred_val, vad_test_data, k=10))\n",
    "                    n25_list.append(NDCG_binary_at_k_batch(pred_val, vad_test_data, k=25))\n",
    "\n",
    "                    r10_list.append(Recall_at_k_batch(pred_val, vad_test_data, k=10))\n",
    "                    r25_list.append(Recall_at_k_batch(pred_val, vad_test_data, k=25))\n",
    "\n",
    "\n",
    "                    n10_list = np.concatenate(n10_list)\n",
    "                    n25_list = np.concatenate(n25_list)\n",
    "\n",
    "                    r10_list = np.concatenate(r10_list)\n",
    "                    r25_list = np.concatenate(r25_list)\n",
    "\n",
    "                    #0cold-\n",
    "                    print(\"lambda1={}\".format(str(lambda1)))\n",
    "                    print(\"lambda2={}\".format(str(lambda2)))\n",
    "                    print(\"lambda3={}\".format(str(lambda3)))\n",
    "\n",
    "                    print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "                    print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "\n",
    "                    print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n25_list), np.nanstd(n25_list) / np.sqrt(len(n25_list))))\n",
    "                    print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r25_list), np.nanstd(r25_list) / np.sqrt(len(r25_list))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
