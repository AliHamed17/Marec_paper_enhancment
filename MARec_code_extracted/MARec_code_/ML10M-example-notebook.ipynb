{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import scipy\n",
    "import pickle\n",
    "import random \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import bottleneck as bn\n",
    "import scipy.stats as ss\n",
    "\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from io import BytesIO \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import sparse\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout, Embedding, Flatten, Dot, MultiHeadAttention, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import L1L2, l1, l2\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ML10M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sparse matrices: sparse matrix of type '<class 'numpy.float64'>\n",
    "train_data = pickle.loads(\"<path>-train.pickle\")\n",
    "cold_test_data = pickle.loads(\"<path>-vad.pickle\")\n",
    "cold_vad_data = pickle.loads(\"<path>-test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(cold_vad_data.shape)\n",
    "print(cold_test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    \n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum() for n in heldout_batch.getnnz(axis=1)])\n",
    "\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metadata features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieId_from_indexes(idx, unique_id_list):\n",
    "    return unique_id_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mapping row id to ML10M id in metadata json\n",
    "unique_id_csv = pd.read_csv('unique_sid_10m.csv', header=None)\n",
    "unique_id_list = unique_id_csv[unique_id_csv.columns[0]].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content = json.loads(\"<path>-ML-10M-metadata.json\")\n",
    "Bimageemb = pickle.loads(\"<path>-image-embeddings.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_actors = []\n",
    "list_genres = []\n",
    "list_directors = []\n",
    "vocabulary_title = []\n",
    "vocabulary_description = []\n",
    "list_tags = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process metatata, json keys ordered by row number, as per unique_id_list\n",
    "for key in json_content: \n",
    "    \n",
    "    data = json_content[key]['title'] \n",
    "    vocabulary_title.extend(data.split())\n",
    "    data = json_content[key]['description']\n",
    "    vocabulary_description.extend(data.split())\n",
    "    list_actors.extend(json_content[key]['actors'])\n",
    "    list_directors.extend(json_content[key]['directors']) \n",
    "    list_genres.extend(json_content[key]['genres'])\n",
    "    list_tags.extend(json_content[key]['tags'])\n",
    "\n",
    "set_actors = set(list_actors)\n",
    "set_directors = set(list_directors)\n",
    "set_genres = set(list_genres)\n",
    "set_vocabulary_title = set(vocabulary_title)\n",
    "set_vocabulary_description = set(vocabulary_description)\n",
    "set_tags = set(list_tags)\n",
    "\n",
    "corpus_title = [json_content[key]['title'] for key in json_content]\n",
    "corpus_description = [json_content[key]['description'] for key in json_content]\n",
    "corpus_actors = [json_content[key]['actors'] for key in json_content]\n",
    "corpus_directors = [json_content[key]['directors'] for key in json_content]\n",
    "corpus_genres = [json_content[key]['genres'] for key in json_content]\n",
    "corpus_tags = [json_content[key]['tags'] for key in json_content]\n",
    "\n",
    "\n",
    "counter_actors = Counter(list_actors)\n",
    "df_actors = pd.DataFrame(list(counter_actors.items()),columns = ['actors','count'])\n",
    "df_actors = df_actors.sort_values(\"count\",ascending=False)\n",
    "# max_features = len(df_actors)//5\n",
    "df_actors= df_actors[df_actors[\"count\"]>=2]\n",
    "# df_actors = df_actors.iloc[:max_features]\n",
    "\n",
    "counter_directors = Counter(list_directors)\n",
    "df_directors = pd.DataFrame(list(counter_directors.items()),columns = ['directors','count'])\n",
    "df_directors = df_directors.sort_values(\"count\",ascending=False)\n",
    "df_directors= df_directors[df_directors[\"count\"]>=2]\n",
    "\n",
    "max_features = 1000\n",
    "pipe_title = Pipeline([('count', CountVectorizer(stop_words='english',max_features=max_features))]).fit(corpus_title)\n",
    "data_title = pipe_title.transform(corpus_title).toarray()\n",
    "# should use transformer embeddings instead\n",
    "data_title = np.load('title.npy')\n",
    "\n",
    "\n",
    "pipe_description = Pipeline([('count', CountVectorizer(stop_words='english',max_features=max_features))]).fit(corpus_description)\n",
    "data_description = pipe_description.transform(corpus_description).toarray()\n",
    "# should use transformer embeddings instead\n",
    "data_description = np.load('description.npy')\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_actors.actors))\n",
    "encoding_actors = mlb.fit_transform(corpus_actors)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = list(df_directors.directors))\n",
    "encoding_directors = mlb.fit_transform(corpus_directors)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoding_genres = mlb.fit_transform(corpus_genres)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoding_tags = mlb.fit_transform(corpus_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Btitle = np.array(data_title)\n",
    "Bdescription = np.array(data_description)\n",
    "Ball = data_all\n",
    "Bgenres = encoding_genres\n",
    "Bactors = np.array(encoding_actors)\n",
    "Bdirectors = np.array(encoding_directors)\n",
    "Btags = encoding_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Btitle.shape)\n",
    "print(Bdescription.shape)\n",
    "print(Bgenres.shape)\n",
    "print(Bactors.shape)\n",
    "print(Bdirectors.shape)\n",
    "print(Btags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Bgenres = scaler.fit_transform(Bgenres)\n",
    "Bactors = scaler.fit_transform(Bactors)\n",
    "Bdirectors = scaler.fit_transform(Bdirectors)\n",
    "Ball = scaler.fit_transform(Ball)\n",
    "Bdescription = scaler.fit_transform(Bdescription)\n",
    "Btitle = scaler.fit_transform(Btitle)\n",
    "Btags = scaler.fit_transform(Btags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(encoding,shrinkage=0.1):\n",
    "    sim1 = encoding.dot(encoding.T)\n",
    "    norm_fi = np.linalg.norm(encoding,axis=1)\n",
    "    sim2 = np.outer(norm_fi,norm_fi)+shrinkage\n",
    "    sim = sim1/sim2\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = cosine_sim(Btitle, shrinkage=50)\n",
    "B2 = cosine_sim(Bdescription, shrinkage=50)\n",
    "B3 = cosine_sim(Bactors, shrinkage=50)\n",
    "B4 = cosine_sim(Bdirectors, shrinkage=50)\n",
    "B5 = cosine_sim(Bgenres, shrinkage=50)\n",
    "B6 = cosine_sim(Bimageemb, shrinkage=50)\n",
    "B7 = cosine_sim(Btags, shrinkage=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = B1.shape[0]\n",
    "for i in range(size):\n",
    "    B1[i,i]=0\n",
    "    B2[i,i]=0\n",
    "    B3[i,i]=0\n",
    "    B4[i,i]=0     \n",
    "    B5[i,i]=0\n",
    "    B6[i,i]=0\n",
    "    B7[i,i]=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn sum of cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B12 = B1*B2\n",
    "B13 = B1*B3\n",
    "B14 = B1*B4\n",
    "B15 = B1*B5\n",
    "B23 = B2*B3\n",
    "B24 = B2*B4\n",
    "B25 = B2*B5\n",
    "B34 = B3*B4\n",
    "B35 = B3*B5\n",
    "B45 = B4*B5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.toarray()\n",
    "X0 = X.dot(B0)\n",
    "X1 = X.dot(B1)\n",
    "X2 = X.dot(B2)\n",
    "X3 = X.dot(B3)\n",
    "X4 = X.dot(B4)\n",
    "X5 = X.dot(B5)\n",
    "X12 = X.dot(B12)\n",
    "X13 = X.dot(B13)\n",
    "X14 = X.dot(B14)\n",
    "X15 = X.dot(B15)\n",
    "X23 = X.dot(B23)\n",
    "X24 = X.dot(B24)\n",
    "X25 = X.dot(B25)\n",
    "X34 = X.dot(B34)\n",
    "X35 = X.dot(B35)\n",
    "X45 = X.dot(B45)\n",
    "\n",
    "print(\"X^{(i)} computed\")\n",
    "\n",
    "X = np.asarray(X).reshape((X.shape[0]*X.shape[1],1))\n",
    "Xtot = np.concatenate((np.asarray(X1).reshape((X1.shape[0]*X1.shape[1],1)),\n",
    "                      np.asarray(X2).reshape((X2.shape[0]*X2.shape[1],1)),\n",
    "                       np.asarray(X3).reshape((X3.shape[0]*X3.shape[1],1)),\n",
    "                      np.asarray(X4).reshape((X4.shape[0]*X4.shape[1],1)),\n",
    "                       np.asarray(X5).reshape((X5.shape[0]*X5.shape[1],1)),\n",
    "                      np.asarray(X12).reshape((X12.shape[0]*X12.shape[1],1)),\n",
    "                       np.asarray(X13).reshape((X13.shape[0]*X13.shape[1],1)),\n",
    "                      np.asarray(X14).reshape((X14.shape[0]*X14.shape[1],1)),\n",
    "                       np.asarray(X15).reshape((X15.shape[0]*X15.shape[1],1)),\n",
    "                      np.asarray(X23).reshape((X23.shape[0]*X23.shape[1],1)),\n",
    "                       np.asarray(X24).reshape((X24.shape[0]*X24.shape[1],1)),\n",
    "                      np.asarray(X25).reshape((X25.shape[0]*X25.shape[1],1)),\n",
    "                       np.asarray(X34).reshape((X34.shape[0]*X34.shape[1],1)),\n",
    "                       np.asarray(X35).reshape((X35.shape[0]*X35.shape[1],1)),\n",
    "                       np.asarray(X45).reshape((X45.shape[0]*X45.shape[1],1))),axis=1)\n",
    "\n",
    "my_array = X.copy()\n",
    "my_array[my_array == 0] = 0.1\n",
    "my_array = my_array.flatten()\n",
    "\n",
    "reg = LinearRegression().fit(Xtot, X, sample_weight=my_array)\n",
    "\n",
    "print(\"linear regression fitted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1bis = reg.coef_[0][0]*X1\n",
    "X2bis = reg.coef_[0][1]*X2\n",
    "X3bis = reg.coef_[0][2]*X3\n",
    "X4bis = reg.coef_[0][3]*X4\n",
    "X5bis = reg.coef_[0][4]*X5\n",
    "X6bis = reg.coef_[0][5]*X12\n",
    "X7bis = reg.coef_[0][6]*X13\n",
    "X8bis = reg.coef_[0][7]*X14\n",
    "X9bis = reg.coef_[0][8]*X15\n",
    "X10bis = reg.coef_[0][9]*X23\n",
    "X11bis = reg.coef_[0][10]*X24\n",
    "X12bis = reg.coef_[0][11]*X25\n",
    "X13bis = reg.coef_[0][12]*X34\n",
    "X14bis = reg.coef_[0][13]*X35\n",
    "X15bis = reg.coef_[0][14]*X45\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = X1bis+X2bis+X3bis+X4bis+X5bis+X6bis+X7bis+X8bis+X9bis+X10bis+X11bis+X12bis+X13bis+X14bis+X15bis#+X16bis#+X17bis\n",
    "values = train_data.nonzero()\n",
    "pred_val[values] =  -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n10_list, r10_list, n25_list, r25_list, n50_list, r50_list, n100_list, r100_list = [], [],[], [],[], [],[], []\n",
    "r10_list.append(Recall_at_k_batch(pred_val,all_test_data, k=10))\n",
    "n10_list.append(NDCG_binary_at_k_batch(pred_val,all_test_data, k=10))\n",
    "r25_list.append(Recall_at_k_batch(pred_val, all_test_data, k=25))\n",
    "n25_list.append(NDCG_binary_at_k_batch(pred_val, all_test_data, k=25))\n",
    "r50_list.append(Recall_at_k_batch(pred_val, all_test_data, k=50))\n",
    "n50_list.append(NDCG_binary_at_k_batch(pred_val, all_test_data, k=50))\n",
    "r100_list.append(Recall_at_k_batch(pred_val, all_test_data, k=100))\n",
    "n100_list.append(NDCG_binary_at_k_batch(pred_val, all_test_data, k=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content similarity with 2nd order regression (B1 to B5, B34)\n",
    "print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "\n",
    "print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r25_list), np.nanstd(r25_list) / np.sqrt(len(r25_list))))\n",
    "print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n25_list), np.nanstd(n25_list) / np.sqrt(len(n25_list))))\n",
    "\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.nanmean(r50_list), np.nanstd(r50_list) / np.sqrt(len(r50_list))))\n",
    "print(\"Test NDCG@50=%.5f (%.5f)\" % (np.nanmean(n50_list), np.nanstd(n50_list) / np.sqrt(len(n50_list))))\n",
    "\n",
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.nanmean(n100_list), np.nanstd(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@100=%.5f (%.5f)\" % (np.nanmean(r100_list), np.nanstd(r100_list) / np.sqrt(len(r100_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.toarray()\n",
    "values = train_data.nonzero()\n",
    "\n",
    "\n",
    "#initialization\n",
    "lambda1s = [700]\n",
    "lambda2s = [60]\n",
    "lambda3s = [1]\n",
    "ks = [0]\n",
    "\n",
    "b = np.average(X,0)\n",
    "xo = np.mean(b)\n",
    "\n",
    "Xtilde = X1bis+X2bis+X3bis+X4bis+X5bis+X6bis+X7bis+X8bis+X9bis+X10bis+X11bis+X12bis+X13bis+X14bis+X15bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ks:\n",
    "    for lambda1 in lambda1s:\n",
    "        for lambda2 in lambda2s:\n",
    "            for lambda3 in lambda3s:\n",
    "\n",
    "                perc_value = 10\n",
    "                rho = 1 \n",
    "                yo = lambda1\n",
    "                y = [lambda1+k*bi-k*xo for bi in b]\n",
    "\n",
    "\n",
    "                #transform vector to diag matrix IR\n",
    "                vector = np.sum(X, axis=0)\n",
    "                percentile = max(np.percentile(vector,perc_value),1)\n",
    "                k = lambda2/percentile\n",
    "                vector_tr = np.zeros(len(vector))\n",
    "                for counter,item in enumerate(vector):\n",
    "                    if item <= percentile:\n",
    "                        vector_tr[counter] = k*(percentile-item)\n",
    "\n",
    "                IR = np.diag(vector_tr)\n",
    "\n",
    "                \n",
    "                #compute P\n",
    "                P0 = rho*X.T.dot(X)+y*np.diag(np.ones(train_data.shape[1]))+X.T.dot(Xtilde).dot(IR)\n",
    "                P = np.linalg.inv(P0)\n",
    "                print(\"P computed\")\n",
    "                del P0\n",
    "\n",
    "                #update of Bk\n",
    "                B_temp = rho*X.T.dot(X)+lambda3*X.T.dot(Xtilde).dot(IR)\n",
    "                B_tilde = P.dot(B_temp)\n",
    "                gamma = np.diag(B_tilde) / np.diag(P)\n",
    "                B_k = B_tilde - P.dot(np.diag(gamma))\n",
    "                B_k = np.asarray(B_k).reshape((B_k.shape[0],B_k.shape[1]))\n",
    "                # del B_temp\n",
    "                # del B_tilde\n",
    "                # del gamma\n",
    "                print(\"B_k updated\")\n",
    "\n",
    "                val_index_min = 0\n",
    "                n10_list, n20_list, n50_list, n100_list, r10_list, r20_list, r50_list, r100_list = [], [], [], [], [], [], [], []\n",
    "\n",
    "                Xtest = X\n",
    "\n",
    "\n",
    "                pred_val = Xtest.dot(B_k)\n",
    "\n",
    "                pred_val[values] = -np.inf\n",
    "                n10_list.append(NDCG_binary_at_k_batch(pred_val, cold_vad_data, k=10))\n",
    "                n20_list.append(NDCG_binary_at_k_batch(pred_val, cold_vad_data, k=25))\n",
    "                n50_list.append(NDCG_binary_at_k_batch(pred_val, cold_vad_data, k=50))\n",
    "                n100_list.append(NDCG_binary_at_k_batch(pred_val, cold_vad_data, k=100))\n",
    "                r10_list.append(Recall_at_k_batch(pred_val, cold_vad_data, k=10))\n",
    "                r20_list.append(Recall_at_k_batch(pred_val, cold_vad_data, k=25))\n",
    "                r50_list.append(Recall_at_k_batch(pred_val, cold_vad_data, k=50))\n",
    "                r100_list.append(Recall_at_k_batch(pred_val, cold_vad_data, k=100))\n",
    "\n",
    "                n10_list = np.concatenate(n10_list)\n",
    "                n20_list = np.concatenate(n20_list)\n",
    "                n50_list = np.concatenate(n50_list)\n",
    "                n100_list = np.concatenate(n100_list)\n",
    "\n",
    "                r10_list = np.concatenate(r10_list)\n",
    "                r20_list = np.concatenate(r20_list)\n",
    "                r50_list = np.concatenate(r50_list)\n",
    "                r100_list = np.concatenate(r100_list)\n",
    "\n",
    "                #0cold-\n",
    "                print(\"lambda1={}\".format(str(lambda1)))\n",
    "                print(\"lambda2={}\".format(str(lambda2)))\n",
    "                print(\"lambda3={}\".format(str(lambda3)))\n",
    "\n",
    "                print(\"Test NDCG@10=%.5f (%.5f)\" % (np.nanmean(n10_list), np.nanstd(n10_list) / np.sqrt(len(n10_list))))\n",
    "                print(\"Test NDCG@25=%.5f (%.5f)\" % (np.nanmean(n20_list), np.nanstd(n20_list) / np.sqrt(len(n20_list))))\n",
    "                print(\"Test NDCG@50=%.5f (%.5f)\" % (np.nanmean(n50_list), np.nanstd(n50_list) / np.sqrt(len(n50_list))))\n",
    "                print(\"Test NDCG@100=%.5f (%.5f)\" % (np.nanmean(n100_list), np.nanstd(n100_list) / np.sqrt(len(n100_list))))\n",
    "                print(\"Test Recall@10=%.5f (%.5f)\" % (np.nanmean(r10_list), np.nanstd(r10_list) / np.sqrt(len(r10_list))))\n",
    "                print(\"Test Recall@25=%.5f (%.5f)\" % (np.nanmean(r20_list), np.nanstd(r20_list) / np.sqrt(len(r20_list))))\n",
    "                print(\"Test Recall@50=%.5f (%.5f)\" % (np.nanmean(r50_list), np.nanstd(r50_list) / np.sqrt(len(r50_list))))\n",
    "                print(\"Test Recall@100=%.5f (%.5f)\" % (np.nanmean(r100_list), np.nanstd(r100_list) / np.sqrt(len(r100_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
