{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARec + CA-Rec / UA-Rec / GE-Rec\n",
    "## Cold-Start Recommendation: Reproduction & Enhancements\n",
    "\n",
    "**Paper:** Monteil et al., RecSys 2024 | **Dataset:** MovieLens HetRec 2011 | **Runtime:** ~20 min on T4\n",
    "\n",
    "---\n",
    "| # | Section | | # | Section |\n",
    "|---|---------|---|---|---------|\n",
    "| 1 | Configuration | | 8 | UA-Rec (Gaussian NLL) |\n",
    "| 2 | Environment | | 9 | GE-Rec (CVAE proxy) |\n",
    "| 3 | Data | | 10 | Pipeline + Ablation |\n",
    "| 4 | Features | | 11 | Paper Comparison |\n",
    "| 5 | MARec Baseline | | 12 | Diagnostics (12 plots) |\n",
    "| 6 | Metrics | | 13 | Export |\n",
    "| 7 | CA-Rec (InfoNCE) | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration (optimized for Colab T4 < 30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as _time\n",
    "_NOTEBOOK_START = _time.time()\n",
    "\n",
    "CONFIG = {\n",
    "    # -- Reproducibility --\n",
    "    'seed': 42,\n",
    "    'n_splits': 3,          # 3 splits (paper uses 10, we trade for speed)\n",
    "    'n_seeds': 2,            # 2 seeds for mean+/-std\n",
    "\n",
    "    # -- Data --\n",
    "    'dataset': 'hetrec',\n",
    "    'cold_train_frac': 0.60,\n",
    "    'cold_val_frac': 0.20,\n",
    "\n",
    "    # -- MARec backbone (Paper Eq. 3-9) --\n",
    "    'lambda1': 1.0,\n",
    "    'lambda0': 0.0,\n",
    "    'alpha': 1.0,\n",
    "    'beta': 100.0,\n",
    "    'delta': 20.0,\n",
    "    'second_order': True,\n",
    "    'fuse_weight': 0.5,\n",
    "\n",
    "    # -- Sentence Transformer --\n",
    "    'use_st': True,\n",
    "    'st_model': 'all-MiniLM-L6-v2',\n",
    "\n",
    "    # -- Enhancement toggles --\n",
    "    'use_ca_rec': True,\n",
    "    'use_ua_rec': True,\n",
    "    'use_ge_rec': True,\n",
    "\n",
    "    # -- CA-Rec (Contrastive Alignment) --\n",
    "    'ca_temperature': 0.07,\n",
    "    'ca_epochs': 15,         # reduced from 25\n",
    "    'ca_lr': 1e-3,           # higher LR for faster convergence\n",
    "    'ca_hidden_dim': 128,    # reduced from 256\n",
    "\n",
    "    # -- UA-Rec (Uncertainty-Aware) --\n",
    "    'ua_epochs': 15,         # reduced from 25\n",
    "    'ua_lr': 1e-3,\n",
    "    'ua_hidden_dim': 128,\n",
    "    'ua_min_logvar': -10.0,\n",
    "    'ua_max_logvar': 2.0,\n",
    "\n",
    "    # -- GE-Rec (CVAE) --\n",
    "    'ge_latent_dim': 32,     # reduced from 64\n",
    "    'ge_hidden_dim': 128,    # reduced from 256\n",
    "    'ge_epochs': 20,         # reduced from 30\n",
    "    'ge_lr': 2e-3,           # higher LR\n",
    "    'ge_kl_warmup': 8,\n",
    "    'ge_kl_weight': 0.01,\n",
    "\n",
    "    # -- Evaluation --\n",
    "    'ks': [10, 25, 50],\n",
    "\n",
    "    # -- Output --\n",
    "    'output_dir': '/content/marec_results',\n",
    "}\n",
    "\n",
    "# Runtime estimate\n",
    "_n_runs = CONFIG['n_splits'] * CONFIG['n_seeds'] * 4  # 4 ablation modes\n",
    "print(f'CONFIG loaded. Estimated pipeline runs: {_n_runs}')\n",
    "print(f'  Splits={CONFIG[\"n_splits\"]}, Seeds={CONFIG[\"n_seeds\"]}')\n",
    "print(f'  CA epochs={CONFIG[\"ca_epochs\"]}, UA={CONFIG[\"ua_epochs\"]}, GE={CONFIG[\"ge_epochs\"]}')\n",
    "print(f'  Hidden dim={CONFIG[\"ca_hidden_dim\"]}, GE latent={CONFIG[\"ge_latent_dim\"]}')\n",
    "print(f'  Enhancements: CA={CONFIG[\"use_ca_rec\"]}, UA={CONFIG[\"use_ua_rec\"]}, GE={CONFIG[\"use_ge_rec\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os, time, warnings, gc, random, math\n",
    "from collections import defaultdict\n",
    "from itertools import product as iprod\n",
    "from pathlib import Path\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "for pkg in ['scipy', 'scikit-learn', 'pandas', 'matplotlib', 'seaborn', 'tqdm']:\n",
    "    install(pkg)\n",
    "try:\n",
    "    import sentence_transformers\n",
    "except ImportError:\n",
    "    install('sentence-transformers')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_t\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('=' * 55)\n",
    "print('  ENVIRONMENT')\n",
    "print('=' * 55)\n",
    "print(f'  Python : {sys.version.split()[0]}')\n",
    "print(f'  PyTorch: {torch.__version__}')\n",
    "print(f'  NumPy  : {np.__version__}')\n",
    "print(f'  Device : {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'  GPU    : {torch.cuda.get_device_name(0)}')\n",
    "    try:\n",
    "        _, mem = torch.cuda.mem_get_info(0)\n",
    "    except Exception:\n",
    "        mem = getattr(torch.cuda.get_device_properties(0), 'total_memory', 0)\n",
    "    if mem:\n",
    "        print(f'  VRAM   : {mem / 1e9:.1f} GB')\n",
    "print('=' * 55)\n",
    "print(f'  Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data: MovieLens HetRec 2011\n",
    "\n",
    "2,107 users x ~6,234 items x 6 metadata types. Cold split: 60/20/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile\n",
    "\n",
    "DATA_DIR = '/content/data/hetrec'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "URL = 'https://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip'\n",
    "\n",
    "def find_file(base, name):\n",
    "    for root, _, files in os.walk(base):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "    return None\n",
    "\n",
    "if find_file(DATA_DIR, 'user_ratedmovies.dat') is None:\n",
    "    zp = os.path.join(DATA_DIR, 'hetrec.zip')\n",
    "    print('Downloading MovieLens HetRec 2011...')\n",
    "    urllib.request.urlretrieve(URL, zp)\n",
    "    with zipfile.ZipFile(zp, 'r') as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    os.remove(zp)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('Data already present.')\n",
    "print(f'Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build interaction matrix + metadata\n",
    "rf = find_file(DATA_DIR, 'user_ratedmovies.dat')\n",
    "if rf is None:\n",
    "    rf = find_file(DATA_DIR, 'user_ratedmovies-timestamps.dat')\n",
    "raw = pd.read_csv(rf, sep='\\t', encoding='latin-1')\n",
    "ratings = raw[['userID', 'movieID']].drop_duplicates()\n",
    "\n",
    "all_users = sorted(ratings['userID'].unique())\n",
    "all_items = sorted(ratings['movieID'].unique())\n",
    "user2idx = {u: i for i, u in enumerate(all_users)}\n",
    "item2idx = {it: i for i, it in enumerate(all_items)}\n",
    "idx2item = {i: it for it, i in item2idx.items()}\n",
    "n_users, n_items = len(all_users), len(all_items)\n",
    "\n",
    "row = ratings['userID'].map(user2idx).values\n",
    "col = ratings['movieID'].map(item2idx).values\n",
    "X = csr_matrix((np.ones(len(ratings)), (row, col)), shape=(n_users, n_items))\n",
    "density = X.nnz / (n_users * n_items) * 100\n",
    "\n",
    "metadata = {}\n",
    "for fname, key, col_name in [\n",
    "    ('movie_genres.dat', 'genres', 'genre'),\n",
    "    ('movie_countries.dat', 'countries', 'country'),\n",
    "]:\n",
    "    f = find_file(DATA_DIR, fname)\n",
    "    if f:\n",
    "        df = pd.read_csv(f, sep='\\t', encoding='latin-1')\n",
    "        metadata[key] = df.groupby('movieID')[col_name].apply(list).to_dict()\n",
    "\n",
    "f = find_file(DATA_DIR, 'movie_directors.dat')\n",
    "if f:\n",
    "    df = pd.read_csv(f, sep='\\t', encoding='latin-1')\n",
    "    c = 'directorName' if 'directorName' in df.columns else 'directorID'\n",
    "    metadata['directors'] = df.groupby('movieID')[c].apply(lambda x: [str(v) for v in x]).to_dict()\n",
    "\n",
    "f = find_file(DATA_DIR, 'movie_actors.dat')\n",
    "if f:\n",
    "    df = pd.read_csv(f, sep='\\t', encoding='latin-1')\n",
    "    c = 'actorName' if 'actorName' in df.columns else 'actorID'\n",
    "    if 'ranking' in df.columns:\n",
    "        df = df[df['ranking'] <= 10]\n",
    "    metadata['actors'] = df.groupby('movieID')[c].apply(lambda x: [str(v) for v in x]).to_dict()\n",
    "\n",
    "f = find_file(DATA_DIR, 'movie_locations.dat')\n",
    "if f:\n",
    "    try:\n",
    "        df = pd.read_csv(f, sep='\\t', encoding='latin-1')\n",
    "        lc = [c for c in df.columns if 'location' in c.lower()][0]\n",
    "        metadata['locations'] = df.groupby('movieID')[lc].apply(lambda x: [str(v) for v in x]).to_dict()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "f = find_file(DATA_DIR, 'movies.dat')\n",
    "if f:\n",
    "    try:\n",
    "        df = pd.read_csv(f, sep='\\t', encoding='latin-1')\n",
    "        idc = [c for c in df.columns if 'id' in c.lower()][0]\n",
    "        if 'year' in df.columns:\n",
    "            metadata['years'] = df.set_index(idc)['year'].to_dict()\n",
    "        if 'title' in df.columns:\n",
    "            metadata['titles'] = df.set_index(idc)['title'].to_dict()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f'Users: {n_users} | Items: {n_items} | Interactions: {X.nnz} | Density: {density:.2f}%')\n",
    "print(f'Metadata: {[k for k in metadata if k != \"titles\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold-start splits\n",
    "def create_cold_splits(X, n_splits, seed, train_frac=0.6, val_frac=0.2):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    ni = X.shape[1]\n",
    "    splits = []\n",
    "    for _ in range(n_splits):\n",
    "        perm = rng.permutation(ni)\n",
    "        nt = int(ni * train_frac)\n",
    "        nv = int(ni * val_frac)\n",
    "        tr = sorted(perm[:nt].tolist())\n",
    "        va = sorted(perm[nt:nt + nv].tolist())\n",
    "        te = sorted(perm[nt + nv:].tolist())\n",
    "        splits.append({\n",
    "            'X_train': X[:, tr], 'X_val': X[:, va], 'X_test': X[:, te],\n",
    "            'train_items': tr, 'val_items': va, 'test_items': te,\n",
    "            'test_users': np.array(X[:, te].sum(axis=1)).flatten() > 0,\n",
    "        })\n",
    "    return splits\n",
    "\n",
    "splits = create_cold_splits(X, CONFIG['n_splits'], CONFIG['seed'],\n",
    "                            CONFIG['cold_train_frac'], CONFIG['cold_val_frac'])\n",
    "s0 = splits[0]\n",
    "assert len(set(s0['train_items']) & set(s0['test_items'])) == 0, 'LEAKAGE!'\n",
    "print(f'Splits: {CONFIG[\"n_splits\"]} | Train: {len(s0[\"train_items\"])} | '\n",
    "      f'Val: {len(s0[\"val_items\"])} | Test: {len(s0[\"test_items\"])}')\n",
    "print(f'Test users with clicks: {s0[\"test_users\"].sum()} / {n_users}')\n",
    "print(f'[OK] No item leakage. Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering (BoW + TF-IDF + Sentence Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrices(metadata, item2idx, n_items):\n",
    "    fmats = {}\n",
    "    for key in ['genres', 'directors', 'actors', 'countries', 'locations']:\n",
    "        if key not in metadata:\n",
    "            continue\n",
    "        mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "        labels = [[] for _ in range(n_items)]\n",
    "        for iid, vals in metadata[key].items():\n",
    "            if iid in item2idx:\n",
    "                labels[item2idx[iid]] = [str(v) for v in vals]\n",
    "        F = mlb.fit_transform(labels)\n",
    "        fmats[key] = csr_matrix(F)\n",
    "        print(f'  {key:<12s}: {F.shape[1]:>5} dims')\n",
    "    if 'titles' in metadata:\n",
    "        texts = [''] * n_items\n",
    "        for iid, t in metadata['titles'].items():\n",
    "            if iid in item2idx:\n",
    "                texts[item2idx[iid]] = str(t)\n",
    "        tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        F = tfidf.fit_transform(texts)\n",
    "        fmats['titles_tfidf'] = csr_matrix(F)\n",
    "        print(f'  {\"titles_tfidf\":<12s}: {F.shape[1]:>5} dims')\n",
    "    if 'years' in metadata:\n",
    "        ya = np.zeros(n_items)\n",
    "        mask = np.zeros(n_items, dtype=bool)\n",
    "        for iid, y in metadata['years'].items():\n",
    "            if iid in item2idx:\n",
    "                ya[item2idx[iid]] = y\n",
    "                mask[item2idx[iid]] = True\n",
    "        if mask.any():\n",
    "            mu, sd = ya[mask].mean(), max(ya[mask].std(), 1)\n",
    "            ya[mask] = (ya[mask] - mu) / sd\n",
    "            fmats['years'] = csr_matrix(ya.reshape(-1, 1))\n",
    "            print(f'  {\"years\":<12s}:     1 dims')\n",
    "    return fmats\n",
    "\n",
    "print('Building BoW features...')\n",
    "feature_matrices = build_feature_matrices(metadata, item2idx, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if CONFIG['use_st']:\n",
    "    print(f'Loading ST: {CONFIG[\"st_model\"]}')\n",
    "    _st = SentenceTransformer(CONFIG['st_model'])\n",
    "    item_texts = []\n",
    "    for idx in range(n_items):\n",
    "        iid = idx2item.get(idx, idx)\n",
    "        parts = []\n",
    "        if 'titles' in metadata and iid in metadata.get('titles', {}):\n",
    "            parts.append(str(metadata['titles'][iid]))\n",
    "        for k in ['genres', 'directors', 'actors']:\n",
    "            if k in metadata and iid in metadata.get(k, {}):\n",
    "                parts.append(', '.join(str(v) for v in metadata[k][iid][:5]))\n",
    "        item_texts.append('. '.join(parts) if parts else 'Unknown')\n",
    "    st_emb = _st.encode(item_texts, show_progress_bar=True, batch_size=256)\n",
    "    feature_matrices['st_emb'] = csr_matrix(st_emb)\n",
    "    print(f'ST embeddings: {st_emb.shape}')\n",
    "    del _st\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('ST disabled.')\n",
    "\n",
    "fmats = feature_matrices\n",
    "total_dim = sum(v.shape[1] for v in fmats.values())\n",
    "print(f'Total: {len(fmats)} feature types, {total_dim} dims')\n",
    "print(f'Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MARec Baseline (EASE + Alignment, Paper Eq. 3-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EASE:\n",
    "    def __init__(self, lambda1=1.0, lambda0=0.0):\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda0 = lambda0\n",
    "        self.B = None\n",
    "\n",
    "    def fit(self, X, F=None, alignment=None):\n",
    "        n = X.shape[1]\n",
    "        G = (X.T @ X).toarray().astype(np.float64)\n",
    "        Gf = G.copy()\n",
    "        if F is not None and self.lambda0 > 0:\n",
    "            FtF = (F.T @ F)\n",
    "            FtF = FtF.toarray() if sp.issparse(FtF) else FtF\n",
    "            Gf += self.lambda0 * FtF.astype(np.float64)\n",
    "        if alignment is None:\n",
    "            XtA = np.zeros_like(G)\n",
    "        else:\n",
    "            Xd = X.T.toarray().astype(np.float64) if sp.issparse(X) else X.T\n",
    "            XtA = Xd @ alignment\n",
    "        P = np.linalg.inv(Gf + self.lambda1 * np.eye(n) + XtA)\n",
    "        Theta = P @ (Gf + XtA)\n",
    "        dP = np.diag(P).copy()\n",
    "        dP[np.abs(dP) < 1e-10] = 1e-10\n",
    "        self.B = Theta - P * (np.diag(Theta) / dP)[None, :]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xd = X.toarray() if sp.issparse(X) else X\n",
    "        return Xd @ self.B\n",
    "\n",
    "\n",
    "class MARecAligner:\n",
    "    def __init__(self, alpha=1.0, beta=100.0, delta=20.0, second_order=True):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.delta = delta\n",
    "        self.second_order = second_order\n",
    "        self.mu = None\n",
    "        self.mu_cross = None\n",
    "        self.names = []\n",
    "\n",
    "    def _d(self, M):\n",
    "        return M.toarray() if sp.issparse(M) else np.asarray(M)\n",
    "\n",
    "    def compute_G(self, fmats):\n",
    "        G_list = []\n",
    "        self.names = list(fmats.keys())\n",
    "        for name in self.names:\n",
    "            Fd = self._d(fmats[name])\n",
    "            norms = np.maximum(np.linalg.norm(Fd, axis=1, keepdims=True), 1e-10)\n",
    "            Fn = Fd / norms\n",
    "            Gk = Fn @ Fn.T\n",
    "            sc = (np.abs(Fd) > 0).sum(1).astype(float)\n",
    "            mx = max(sc.max(), 1)\n",
    "            sf = sc / mx\n",
    "            mask = np.outer(sf, sf)\n",
    "            Gk = Gk * mask / (mask + self.delta / (mx + 1))\n",
    "            G_list.append(Gk)\n",
    "        return G_list\n",
    "\n",
    "    def compute_DR(self, Xtr):\n",
    "        clicks = np.array(Xtr.sum(0)).flatten()\n",
    "        p = max(np.percentile(clicks[clicks > 0], 10) if (clicks > 0).any() else 1, 1)\n",
    "        d = np.where(clicks <= p, (self.beta / p) * np.maximum(p - clicks, 0), 0.0)\n",
    "        return np.diag(d)\n",
    "\n",
    "    def fit_weights(self, Xtr, G_list):\n",
    "        N = len(G_list)\n",
    "        XtX = (Xtr.T @ Xtr).toarray().flatten()\n",
    "        rng = np.random.RandomState(0)\n",
    "        idx = rng.choice(len(XtX), min(15000, len(XtX)), replace=False)\n",
    "        xs = XtX[idx]\n",
    "        self.mu = np.ones(N)\n",
    "        best = -1e9\n",
    "        grid = [0.0, 0.5, 1.0, 2.0, 5.0] if N <= 4 else [0.0, 1.0, 3.0]\n",
    "        for combo in iprod(grid, repeat=N):\n",
    "            mu = np.array(combo)\n",
    "            if mu.sum() == 0:\n",
    "                continue\n",
    "            Gc = sum(mu[k] * G_list[k] for k in range(N))\n",
    "            c = np.corrcoef(Gc.flatten()[idx], xs)[0, 1]\n",
    "            if not np.isnan(c) and c > best:\n",
    "                best = c\n",
    "                self.mu = mu.copy()\n",
    "        self.mu_cross = np.zeros((N, N))\n",
    "        if self.second_order and N >= 2:\n",
    "            for i in range(N):\n",
    "                for j in range(i + 1, N):\n",
    "                    for w in [0.0, 0.5, 1.0, 2.0]:\n",
    "                        Gc = sum(self.mu[k] * G_list[k] for k in range(N))\n",
    "                        Gc = Gc + w * (G_list[i] @ G_list[j])\n",
    "                        c = np.corrcoef(Gc.flatten()[idx], xs)[0, 1]\n",
    "                        if not np.isnan(c) and c > best:\n",
    "                            best = c\n",
    "                            self.mu_cross[i, j] = w\n",
    "\n",
    "    def combine_G(self, G_list):\n",
    "        N = len(G_list)\n",
    "        G = sum(self.mu[k] * G_list[k] for k in range(N))\n",
    "        if self.second_order:\n",
    "            for i in range(N):\n",
    "                for j in range(i + 1, N):\n",
    "                    if self.mu_cross[i, j] > 0:\n",
    "                        G = G + self.mu_cross[i, j] * (G_list[i] @ G_list[j])\n",
    "        return G\n",
    "\n",
    "    def cross_sim(self, fmats, cold_items, warm_items):\n",
    "        cross = []\n",
    "        for name in self.names:\n",
    "            Fc = self._d(fmats[name][cold_items])\n",
    "            Fw = self._d(fmats[name][warm_items])\n",
    "            cross.append(normalize(Fc) @ normalize(Fw).T)\n",
    "        N = len(cross)\n",
    "        G = sum(self.mu[k] * cross[k] for k in range(N))\n",
    "        if self.second_order:\n",
    "            for i in range(N):\n",
    "                for j in range(i + 1, N):\n",
    "                    if self.mu_cross[i, j] > 0:\n",
    "                        G = G + self.mu_cross[i, j] * cross[i] * cross[j]\n",
    "        return G\n",
    "\n",
    "print('EASE + MARecAligner defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation Metrics (Paper Eq. 12-13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(scores, X_test, ks=(10, 25, 50), user_mask=None):\n",
    "    Xt = X_test.toarray() if sp.issparse(X_test) else X_test\n",
    "    res = {}\n",
    "    for k in ks:\n",
    "        res[f'hr@{k}'] = 0.0\n",
    "        res[f'ndcg@{k}'] = 0.0\n",
    "    n_eval = 0\n",
    "    for u in range(Xt.shape[0]):\n",
    "        if user_mask is not None and not user_mask[u]:\n",
    "            continue\n",
    "        true = np.where(Xt[u] > 0)[0]\n",
    "        if len(true) == 0:\n",
    "            continue\n",
    "        ranked = np.argsort(scores[u])[::-1]\n",
    "        ts = set(true)\n",
    "        for k in ks:\n",
    "            topk = ranked[:k]\n",
    "            hits = sum(1 for i in topk if i in ts)\n",
    "            res[f'hr@{k}'] += hits / min(k, len(true))\n",
    "            dcg = sum(1.0 / np.log2(r + 2) for r, i in enumerate(topk) if i in ts)\n",
    "            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(k, len(true))))\n",
    "            res[f'ndcg@{k}'] += (dcg / idcg) if idcg > 0 else 0.0\n",
    "        n_eval += 1\n",
    "    if n_eval > 0:\n",
    "        for k in res:\n",
    "            res[k] /= n_eval\n",
    "    res['n_eval'] = n_eval\n",
    "    return res\n",
    "\n",
    "def evaluate_coverage(scores, k=50):\n",
    "    recs = set()\n",
    "    for u in range(scores.shape[0]):\n",
    "        recs.update(np.argsort(scores[u])[-k:].tolist())\n",
    "    return len(recs) / scores.shape[1]\n",
    "\n",
    "print('Metrics: HR@K, NDCG@K, Coverage@K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. CA-Rec: Contrastive Alignment (InfoNCE)\n",
    "\n",
    "Replace MSE alignment with InfoNCE contrastive loss.\n",
    "Metadata embedding should be close to its matching interaction embedding and far from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARec(nn.Module):\n",
    "    # Contrastive Alignment: metadata_proj & interact_proj -> InfoNCE\n",
    "    def __init__(self, meta_dim, interact_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.meta_proj = nn.Sequential(\n",
    "            nn.Linear(meta_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.interact_proj = nn.Sequential(\n",
    "            nn.Linear(interact_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, meta_feats, interact_feats, temperature=0.07):\n",
    "        m = F_t.normalize(self.meta_proj(meta_feats), dim=-1)\n",
    "        v = F_t.normalize(self.interact_proj(interact_feats), dim=-1)\n",
    "        logits = m @ v.T / temperature\n",
    "        labels = torch.arange(m.shape[0], device=m.device)\n",
    "        loss = F_t.cross_entropy(logits, labels)\n",
    "        return loss, m\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def project_meta(self, meta_feats):\n",
    "        return F_t.normalize(self.meta_proj(meta_feats), dim=-1)\n",
    "\n",
    "print('CA-Rec defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. UA-Rec: Uncertainty-Aware Alignment (Gaussian NLL)\n",
    "\n",
    "Alignment head outputs (mu, sigma^2). Noisy metadata -> high sigma^2 -> weak pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UARec(nn.Module):\n",
    "    # Uncertainty-Aware: shared -> (mu, logvar) -> Gaussian NLL\n",
    "    def __init__(self, meta_dim, target_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(meta_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, target_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, target_dim)\n",
    "\n",
    "    def forward(self, meta_feats, targets, min_lv=-10.0, max_lv=2.0):\n",
    "        h = self.shared(meta_feats)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h).clamp(min_lv, max_lv)\n",
    "        var = logvar.exp()\n",
    "        nll = (1.0 / var) * (targets - mu).pow(2) + logvar\n",
    "        return nll.mean(), mu, var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, meta_feats, min_lv=-10.0, max_lv=2.0):\n",
    "        h = self.shared(meta_feats)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h).clamp(min_lv, max_lv)\n",
    "        return mu, logvar.exp()\n",
    "\n",
    "print('UA-Rec defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. GE-Rec: Generative Embeddings (Conditional VAE)\n",
    "\n",
    "Train CVAE: encode (v_i, m_i) -> z, decode (z, m_i) -> v_hat.\n",
    "For cold items: z ~ N(0,I), decode(z, m_cold) -> proxy interaction embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GERec(nn.Module):\n",
    "    # CVAE: encoder (v,m)->z, decoder (z,m)->v_hat\n",
    "    def __init__(self, interact_dim, meta_dim, latent_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        enc_in = interact_dim + meta_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(enc_in, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "        )\n",
    "        self.fc_mu_z = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar_z = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        dec_in = latent_dim + meta_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dec_in, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim), nn.GELU(),\n",
    "            nn.Linear(hidden_dim, interact_dim),\n",
    "        )\n",
    "\n",
    "    def encode(self, v, m):\n",
    "        h = self.encoder(torch.cat([v, m], dim=-1))\n",
    "        return self.fc_mu_z(h), self.fc_logvar_z(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.exp(0.5 * logvar) * torch.randn_like(logvar)\n",
    "        return mu\n",
    "\n",
    "    def decode(self, z, m):\n",
    "        return self.decoder(torch.cat([z, m], dim=-1))\n",
    "\n",
    "    def forward(self, v, m, kl_weight=0.01):\n",
    "        mu_z, logvar_z = self.encode(v, m)\n",
    "        z = self.reparameterize(mu_z, logvar_z)\n",
    "        v_hat = self.decode(z, m)\n",
    "        recon = F_t.mse_loss(v_hat, v)\n",
    "        kl = -0.5 * torch.mean(1 + logvar_z - mu_z.pow(2) - logvar_z.exp())\n",
    "        return recon + kl_weight * kl, recon, kl, v_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, m_cold):\n",
    "        B = m_cold.shape[0]\n",
    "        z = torch.randn(B, self.latent_dim, device=m_cold.device)\n",
    "        return self.decode(z, m_cold)\n",
    "\n",
    "print('GE-Rec defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Pipeline & Ablation\n",
    "\n",
    "Modes: baseline -> +CA -> +CA+UA -> Full (+CA+UA+GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cat_feats(fmats, items):\n",
    "    return sp.hstack([v[items] for v in fmats.values()]).tocsr()\n",
    "\n",
    "def get_feats(fmats, items):\n",
    "    return _cat_feats(fmats, items).toarray().astype(np.float32)\n",
    "\n",
    "def run_pipeline(split, fmats, cfg, mode='full'):\n",
    "    tr = split['train_items']\n",
    "    te = split['test_items']\n",
    "    Xtr = split['X_train']\n",
    "    Xte = split['X_test']\n",
    "    te_mask = split['test_users']\n",
    "    ks = cfg['ks']\n",
    "    Xtr_d = Xtr.toarray().astype(np.float64)\n",
    "    logs = {'ca': [], 'ua': [], 'ge_recon': [], 'ge_kl': []}\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: MARec baseline (always runs)\n",
    "    fmats_tr = {k: v[tr] for k, v in fmats.items()}\n",
    "    aligner = MARecAligner(cfg['alpha'], cfg['beta'], cfg['delta'], cfg['second_order'])\n",
    "    G_list = aligner.compute_G(fmats_tr)\n",
    "    aligner.fit_weights(Xtr, G_list)\n",
    "    G_comb = aligner.combine_G(G_list)\n",
    "    DR = aligner.compute_DR(Xtr)\n",
    "    alignment = aligner.alpha * Xtr_d @ G_comb @ DR\n",
    "\n",
    "    ease = EASE(cfg['lambda1'], cfg.get('lambda0', 0))\n",
    "    ease.fit(Xtr, alignment=alignment)\n",
    "    warm_scores = ease.predict(Xtr)\n",
    "    cross_G = aligner.cross_sim(fmats, te, tr)\n",
    "    cold_ease = warm_scores @ cross_G.T\n",
    "    direct = aligner.alpha * Xtr_d @ cross_G.T\n",
    "    fw = cfg.get('fuse_weight', 0.5)\n",
    "    baseline = fw * cold_ease + (1 - fw) * direct\n",
    "    results['MARec Baseline'] = evaluate(baseline, Xte, ks, te_mask)\n",
    "    if mode == 'baseline':\n",
    "        return results, logs, baseline\n",
    "\n",
    "    # Prepare tensors (shared across enhancements)\n",
    "    meta_cold = torch.tensor(get_feats(fmats, te), device=DEVICE)\n",
    "    meta_warm = torch.tensor(get_feats(fmats, tr), device=DEVICE)\n",
    "    XtX = (Xtr.T @ Xtr).toarray().astype(np.float32)\n",
    "    interact_warm = torch.tensor(XtX, device=DEVICE)\n",
    "    meta_dim = meta_cold.shape[1]\n",
    "    interact_dim = interact_warm.shape[1]\n",
    "\n",
    "    # Step 2: CA-Rec\n",
    "    ca_scores = baseline.copy()\n",
    "    if cfg.get('use_ca_rec') and mode in ['+ca', '+ca+ua', 'full']:\n",
    "        ca = CARec(meta_dim, interact_dim, cfg['ca_hidden_dim']).to(DEVICE)\n",
    "        opt = torch.optim.Adam(ca.parameters(), lr=cfg['ca_lr'])\n",
    "        ca.train()\n",
    "        for ep in range(cfg['ca_epochs']):\n",
    "            bs = min(512, meta_warm.shape[0])\n",
    "            idx = torch.randperm(meta_warm.shape[0], device=DEVICE)[:bs]\n",
    "            loss, _ = ca(meta_warm[idx], interact_warm[idx], cfg['ca_temperature'])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            logs['ca'].append(loss.item())\n",
    "        ca.eval()\n",
    "        cold_proj = ca.project_meta(meta_cold).cpu().numpy()\n",
    "        warm_proj = ca.project_meta(meta_warm).cpu().numpy()\n",
    "        ca_sim = cold_proj @ warm_proj.T\n",
    "        ca_transfer = Xtr_d @ ca_sim.T.astype(np.float64)\n",
    "        ca_scores = 0.6 * baseline + 0.4 * ca_transfer\n",
    "        results['+CA-Rec'] = evaluate(ca_scores, Xte, ks, te_mask)\n",
    "        del ca, opt\n",
    "        if mode == '+ca':\n",
    "            return results, logs, ca_scores\n",
    "\n",
    "    # Step 3: UA-Rec\n",
    "    ua_scores = ca_scores.copy()\n",
    "    if cfg.get('use_ua_rec') and mode in ['+ca+ua', 'full']:\n",
    "        ua = UARec(meta_dim, interact_dim, cfg['ua_hidden_dim']).to(DEVICE)\n",
    "        opt = torch.optim.Adam(ua.parameters(), lr=cfg['ua_lr'])\n",
    "        ua.train()\n",
    "        for ep in range(cfg['ua_epochs']):\n",
    "            bs = min(512, meta_warm.shape[0])\n",
    "            idx = torch.randperm(meta_warm.shape[0], device=DEVICE)[:bs]\n",
    "            loss, _, _ = ua(meta_warm[idx], interact_warm[idx],\n",
    "                            cfg['ua_min_logvar'], cfg['ua_max_logvar'])\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            logs['ua'].append(loss.item())\n",
    "        ua.eval()\n",
    "        mu_cold, var_cold = ua.predict(meta_cold, cfg['ua_min_logvar'], cfg['ua_max_logvar'])\n",
    "        confidence = (1.0 / var_cold.mean(dim=1, keepdim=True)).cpu().numpy()\n",
    "        mu_np = mu_cold.cpu().numpy()\n",
    "        mu_warm, _ = ua.predict(meta_warm, cfg['ua_min_logvar'], cfg['ua_max_logvar'])\n",
    "        ua_sim = normalize(mu_np) @ normalize(mu_warm.cpu().numpy()).T\n",
    "        ua_transfer = Xtr_d @ ua_sim.T.astype(np.float64)\n",
    "        conf_norm = confidence / (confidence.mean() + 1e-10)\n",
    "        ua_scores = 0.5 * ca_scores + 0.3 * ua_transfer * conf_norm.T + 0.2 * baseline\n",
    "        results['+CA+UA-Rec'] = evaluate(ua_scores, Xte, ks, te_mask)\n",
    "        del ua, opt\n",
    "        if mode == '+ca+ua':\n",
    "            return results, logs, ua_scores\n",
    "\n",
    "    # Step 4: GE-Rec\n",
    "    if cfg.get('use_ge_rec') and mode == 'full':\n",
    "        ge = GERec(interact_dim, meta_dim, cfg['ge_latent_dim'],\n",
    "                   cfg['ge_hidden_dim']).to(DEVICE)\n",
    "        opt = torch.optim.Adam(ge.parameters(), lr=cfg['ge_lr'])\n",
    "        ge.train()\n",
    "        for ep in range(cfg['ge_epochs']):\n",
    "            kl_w = min(1.0, ep / max(cfg['ge_kl_warmup'], 1)) * cfg['ge_kl_weight']\n",
    "            bs = min(512, meta_warm.shape[0])\n",
    "            idx = torch.randperm(meta_warm.shape[0], device=DEVICE)[:bs]\n",
    "            loss, recon, kl, _ = ge(interact_warm[idx], meta_warm[idx], kl_w)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            logs['ge_recon'].append(recon.item())\n",
    "            logs['ge_kl'].append(kl.item())\n",
    "        ge.eval()\n",
    "        v_cold_proxy = ge.generate(meta_cold).cpu().numpy()\n",
    "        ge_sim = normalize(v_cold_proxy) @ normalize(XtX).T\n",
    "        ge_transfer = Xtr_d @ ge_sim.T.astype(np.float64)\n",
    "        final = 0.4 * ua_scores + 0.35 * ge_transfer + 0.25 * baseline\n",
    "        results['+CA+UA+GE-Rec'] = evaluate(final, Xte, ks, te_mask)\n",
    "        del ge, opt\n",
    "        return results, logs, final\n",
    "\n",
    "    return results, logs, ua_scores if '+CA+UA-Rec' in results else ca_scores\n",
    "\n",
    "print('Pipeline defined.')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f'Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  RUN FULL ABLATION\n",
    "# ============================================================\n",
    "def run_ablation(splits, fmats, cfg, seeds):\n",
    "    modes = ['baseline', '+ca', '+ca+ua', 'full']\n",
    "    key_map = {\n",
    "        'baseline': 'MARec Baseline',\n",
    "        '+ca': '+CA-Rec',\n",
    "        '+ca+ua': '+CA+UA-Rec',\n",
    "        'full': '+CA+UA+GE-Rec',\n",
    "    }\n",
    "    all_results = {m: defaultdict(list) for m in modes}\n",
    "    all_logs = {}\n",
    "    total = len(seeds) * len(splits) * len(modes)\n",
    "    pbar = tqdm(total=total, desc='Ablation')\n",
    "\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)\n",
    "        for split in splits:\n",
    "            for mode in modes:\n",
    "                cfg_run = dict(cfg)\n",
    "                cfg_run['use_ca_rec'] = mode in ['+ca', '+ca+ua', 'full']\n",
    "                cfg_run['use_ua_rec'] = mode in ['+ca+ua', 'full']\n",
    "                cfg_run['use_ge_rec'] = mode == 'full'\n",
    "                res, logs, _ = run_pipeline(split, fmats, cfg_run, mode=mode)\n",
    "                target = key_map[mode]\n",
    "                if target in res:\n",
    "                    for k, v in res[target].items():\n",
    "                        if k != 'n_eval':\n",
    "                            all_results[mode][k].append(v)\n",
    "                if mode == 'full':\n",
    "                    all_logs = logs\n",
    "                pbar.update(1)\n",
    "                # Memory cleanup between runs\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    pbar.close()\n",
    "\n",
    "    summary = {}\n",
    "    for mode in modes:\n",
    "        key = key_map[mode]\n",
    "        summary[key] = {}\n",
    "        for k, vals in all_results[mode].items():\n",
    "            summary[key][k] = float(np.mean(vals))\n",
    "            summary[key][k + '_std'] = float(np.std(vals))\n",
    "    return summary, all_logs\n",
    "\n",
    "print('=' * 60)\n",
    "print('  RUNNING ABLATION')\n",
    "print(f'  {CONFIG[\"n_splits\"]} splits x {CONFIG[\"n_seeds\"]} seeds x 4 modes = '\n",
    "      f'{CONFIG[\"n_splits\"] * CONFIG[\"n_seeds\"] * 4} runs')\n",
    "print('=' * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "seeds = [CONFIG['seed'] + i * 111 for i in range(CONFIG['n_seeds'])]\n",
    "abl_summary, abl_logs = run_ablation(splits, fmats, CONFIG, seeds)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "models_order = ['MARec Baseline', '+CA-Rec', '+CA+UA-Rec', '+CA+UA+GE-Rec']\n",
    "\n",
    "print(f'\\nAblation done in {elapsed:.0f}s ({elapsed/60:.1f} min)')\n",
    "print('\\n' + '=' * 92)\n",
    "ks = CONFIG['ks']\n",
    "hdr = f'{\"Model\":<22s}'\n",
    "for k in ks:\n",
    "    hdr += f'{\"HR@\" + str(k):>14s}{\"NDCG@\" + str(k):>14s}'\n",
    "print(hdr)\n",
    "print('-' * 92)\n",
    "for m in models_order:\n",
    "    if m not in abl_summary:\n",
    "        continue\n",
    "    r = abl_summary[m]\n",
    "    row = f'{m:<22s}'\n",
    "    for k in ks:\n",
    "        hr = r.get(f'hr@{k}', 0)\n",
    "        hs = r.get(f'hr@{k}_std', 0)\n",
    "        nd = r.get(f'ndcg@{k}', 0)\n",
    "        ns = r.get(f'ndcg@{k}_std', 0)\n",
    "        row += f'{hr:.4f}+/-{hs:.3f} {nd:.4f}+/-{ns:.3f} '\n",
    "    print(row)\n",
    "print('=' * 92)\n",
    "print(f'Total elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Paper Comparison (Table 3 - MovieLens HetRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER = {\n",
    "    'ItemKNNCF':     {'hr@10': 0.1175, 'ndcg@10': 0.1335},\n",
    "    'CLCRec':        {'hr@10': 0.0815, 'ndcg@10': 0.0763},\n",
    "    'EQUAL':         {'hr@10': 0.1310, 'ndcg@10': 0.1470},\n",
    "    'NFC':           {'hr@10': 0.1904, 'ndcg@10': 0.2076},\n",
    "    'MARec (paper)': {'hr@10': 0.2928, 'ndcg@10': 0.3071},\n",
    "}\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'{\"Model\":<22s}{\"HR@10\":>12s}{\"NDCG@10\":>12s}{\"Lift\":>12s}')\n",
    "print('-' * 70)\n",
    "print('  Paper Table 3:')\n",
    "for n, r in PAPER.items():\n",
    "    print(f'    {n:<18s}{r[\"hr@10\"]:>12.4f}{r[\"ndcg@10\"]:>12.4f}')\n",
    "\n",
    "print('  Our results:')\n",
    "pm = PAPER['MARec (paper)']\n",
    "for m in models_order:\n",
    "    if m not in abl_summary:\n",
    "        continue\n",
    "    r = abl_summary[m]\n",
    "    hr = r.get('hr@10', 0)\n",
    "    lift = (hr - pm['hr@10']) / pm['hr@10'] * 100\n",
    "    print(f'    {m:<18s}{hr:>12.4f}{r.get(\"ndcg@10\", 0):>12.4f}{lift:>+10.1f}%')\n",
    "print('=' * 70)\n",
    "print('Note: paper uses 10 splits + 500 bootstrap, we use',\n",
    "      CONFIG['n_splits'], 'splits +', CONFIG['n_seeds'], 'seeds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Diagnostics (12-plot grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single diagnostic run for detailed analysis\n",
    "print('Diagnostic run (single split)...')\n",
    "set_seed(CONFIG['seed'])\n",
    "s0 = splits[0]\n",
    "res_diag, logs_diag, scores_diag = run_pipeline(s0, fmats, CONFIG, mode='full')\n",
    "cfg_base = dict(CONFIG)\n",
    "cfg_base['use_ca_rec'] = False\n",
    "cfg_base['use_ua_rec'] = False\n",
    "cfg_base['use_ge_rec'] = False\n",
    "_, _, scores_base = run_pipeline(s0, fmats, cfg_base, mode='baseline')\n",
    "\n",
    "cold_feats = get_feats(fmats, s0['test_items'])\n",
    "warm_feats = get_feats(fmats, s0['train_items'])\n",
    "print(f'Done. Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "sns.set_style('whitegrid')\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(4, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# -- Row 1: Ablation bars --\n",
    "mp = [m for m in models_order if m in abl_summary]\n",
    "lbl = ['Baseline', '+CA', '+CA+UA', 'Full'][:len(mp)]\n",
    "x = np.arange(len(mp))\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "hr10 = [abl_summary[m].get('hr@10', 0) for m in mp]\n",
    "nd10 = [abl_summary[m].get('ndcg@10', 0) for m in mp]\n",
    "ax.bar(x - 0.18, hr10, 0.35, label='HR@10', color='#e74c3c', alpha=0.85)\n",
    "ax.bar(x + 0.18, nd10, 0.35, label='NDCG@10', color='#3498db', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(lbl, fontsize=9)\n",
    "ax.set_title('Ablation @10', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "for i, (h, n) in enumerate(zip(hr10, nd10)):\n",
    "    ax.text(i - 0.18, h + 0.002, f'{h:.3f}', ha='center', fontsize=7)\n",
    "    ax.text(i + 0.18, n + 0.002, f'{n:.3f}', ha='center', fontsize=7)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "hr50 = [abl_summary[m].get('hr@50', 0) for m in mp]\n",
    "nd50 = [abl_summary[m].get('ndcg@50', 0) for m in mp]\n",
    "ax.bar(x - 0.18, hr50, 0.35, label='HR@50', color='#9b59b6', alpha=0.85)\n",
    "ax.bar(x + 0.18, nd50, 0.35, label='NDCG@50', color='#e67e22', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(lbl, fontsize=9)\n",
    "ax.set_title('Ablation @50', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 2])\n",
    "b10 = abl_summary.get('MARec Baseline', {}).get('hr@10', 1e-9)\n",
    "lifts = [(abl_summary[m].get('hr@10', 0) - b10) / max(b10, 1e-9) * 100 for m in mp]\n",
    "cs = ['#2ecc71' if l >= 0 else '#e74c3c' for l in lifts]\n",
    "ax.barh(lbl, lifts, color=cs, alpha=0.85)\n",
    "ax.set_xlabel('% Lift over Baseline')\n",
    "ax.set_title('HR@10 Lift', fontweight='bold')\n",
    "ax.axvline(0, color='k', lw=0.5)\n",
    "\n",
    "# -- Row 2: Training losses --\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "if logs_diag.get('ca'):\n",
    "    ax.plot(logs_diag['ca'], color='#8e44ad', lw=1.5)\n",
    "    ax.set_title('CA-Rec: InfoNCE Loss', fontweight='bold')\n",
    "    ax.set_xlabel('Step')\n",
    "\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "if logs_diag.get('ua'):\n",
    "    ax.plot(logs_diag['ua'], color='#e67e22', lw=1.5)\n",
    "    ax.set_title('UA-Rec: Gaussian NLL', fontweight='bold')\n",
    "    ax.set_xlabel('Step')\n",
    "\n",
    "ax = fig.add_subplot(gs[1, 2])\n",
    "if logs_diag.get('ge_recon'):\n",
    "    ax.plot(logs_diag['ge_recon'], label='Recon', color='#3498db')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(logs_diag['ge_kl'], label='KL', color='#e74c3c', ls='--')\n",
    "    ax.set_title('GE-Rec: CVAE Losses', fontweight='bold')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax2.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# -- Row 3: PCA + NN --\n",
    "ax = fig.add_subplot(gs[2, 0])\n",
    "n_show = min(300, len(warm_feats), len(cold_feats))\n",
    "combined = np.vstack([warm_feats[:n_show], cold_feats[:n_show]])\n",
    "pca = PCA(n_components=2)\n",
    "emb2d = pca.fit_transform(combined)\n",
    "ax.scatter(emb2d[:n_show, 0], emb2d[:n_show, 1], c='#3498db', alpha=0.3, s=10, label='Warm')\n",
    "ax.scatter(emb2d[n_show:, 0], emb2d[n_show:, 1], c='#e74c3c', alpha=0.5, s=15, marker='x', label='Cold')\n",
    "ax.set_title('PCA: Warm vs Cold Features', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "\n",
    "ax = fig.add_subplot(gs[2, 1])\n",
    "n_u = min(400, scores_base.shape[0])\n",
    "combined_s = np.vstack([scores_base[:n_u], scores_diag[:n_u]])\n",
    "pca_s = PCA(n_components=2)\n",
    "s2d = pca_s.fit_transform(combined_s)\n",
    "ax.scatter(s2d[:n_u, 0], s2d[:n_u, 1], c='#95a5a6', alpha=0.3, s=8, label='Baseline')\n",
    "ax.scatter(s2d[n_u:, 0], s2d[n_u:, 1], c='#2ecc71', alpha=0.3, s=8, label='Full')\n",
    "ax.set_title('PCA: Baseline vs Full Scores', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "ax = fig.add_subplot(gs[2, 2])\n",
    "nn_model = NearestNeighbors(n_neighbors=5, metric='cosine').fit(warm_feats)\n",
    "d_raw, _ = nn_model.kneighbors(cold_feats[:min(500, len(cold_feats))])\n",
    "ax.hist(d_raw.mean(axis=1), bins=30, alpha=0.6, label='Feature dist', color='#e74c3c', density=True)\n",
    "ax.set_xlabel('Mean Cosine Dist to 5-NN')\n",
    "ax.set_title('Cold-to-Warm NN Distance', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# -- Row 4: Coverage + HR curves + Paper --\n",
    "ax = fig.add_subplot(gs[3, 0])\n",
    "cov_ks = [10, 25, 50]\n",
    "cov_b = [evaluate_coverage(scores_base, k) for k in cov_ks]\n",
    "cov_f = [evaluate_coverage(scores_diag, k) for k in cov_ks]\n",
    "ax.plot(cov_ks, cov_b, 'o-', label='Baseline', color='#e74c3c', lw=2)\n",
    "ax.plot(cov_ks, cov_f, 's-', label='Full', color='#2ecc71', lw=2)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_title('Coverage@K', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "ax = fig.add_subplot(gs[3, 1])\n",
    "for mn, c in [('MARec Baseline', '#e74c3c'), ('+CA+UA+GE-Rec', '#2ecc71')]:\n",
    "    if mn in abl_summary:\n",
    "        vals = [abl_summary[mn].get(f'hr@{k}', 0) for k in ks]\n",
    "        ax.plot(ks, vals, 'o-', label=mn, lw=2, color=c, markersize=8)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('HR@K')\n",
    "ax.set_xticks(ks)\n",
    "ax.set_title('HR@K Curves', fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "ax = fig.add_subplot(gs[3, 2])\n",
    "pn = list(PAPER.keys())\n",
    "phr = [PAPER[n]['hr@10'] for n in pn]\n",
    "our_best = abl_summary.get('+CA+UA+GE-Rec', abl_summary.get('MARec Baseline', {}))\n",
    "all_n = pn + ['Ours (Full)']\n",
    "all_h = phr + [our_best.get('hr@10', 0)]\n",
    "colors = ['#95a5a6'] * len(pn) + ['#2ecc71']\n",
    "ax.barh(all_n, all_h, color=colors, alpha=0.85)\n",
    "ax.set_xlabel('HR@10')\n",
    "ax.set_title('vs Paper Baselines', fontweight='bold')\n",
    "\n",
    "fig.suptitle('MARec + CA-Rec / UA-Rec / GE-Rec: Diagnostics',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'diagnostics.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Elapsed: {time.time() - _NOTEBOOK_START:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import json as json_mod\n",
    "\n",
    "out = CONFIG['output_dir']\n",
    "os.makedirs(out, exist_ok=True)\n",
    "\n",
    "# Ablation CSV\n",
    "rows = []\n",
    "for m, r in abl_summary.items():\n",
    "    rows.append({'model': m, **{k: round(v, 6) for k, v in r.items()}})\n",
    "pd.DataFrame(rows).to_csv(os.path.join(out, 'ablation_results.csv'), index=False)\n",
    "\n",
    "# Config JSON\n",
    "with open(os.path.join(out, 'config.json'), 'w') as f:\n",
    "    json_mod.dump({k: str(v) if not isinstance(v, (int, float, bool, str, list)) else v\n",
    "                   for k, v in CONFIG.items()}, f, indent=2)\n",
    "\n",
    "# Zip\n",
    "shutil.make_archive(out, 'zip', out)\n",
    "\n",
    "total_time = time.time() - _NOTEBOOK_START\n",
    "print(f'\\nResults saved to {out}.zip')\n",
    "print(f'Files: ablation_results.csv, config.json, diagnostics.png')\n",
    "print(f'\\nTotal runtime: {total_time:.0f}s ({total_time/60:.1f} min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time() - _NOTEBOOK_START\n",
    "print('=' * 70)\n",
    "print('  MARec + CA-Rec / UA-Rec / GE-Rec  --  FINAL RESULTS')\n",
    "print('=' * 70)\n",
    "print(f'  Dataset: HetRec ({n_users} users, {n_items} items)')\n",
    "print(f'  Config:  {CONFIG[\"n_splits\"]} splits, {CONFIG[\"n_seeds\"]} seeds')\n",
    "print(f'  Runtime: {total_time:.0f}s ({total_time/60:.1f} min)')\n",
    "print()\n",
    "for m in models_order:\n",
    "    if m not in abl_summary:\n",
    "        continue\n",
    "    r = abl_summary[m]\n",
    "    parts = []\n",
    "    for k in ['hr@10', 'ndcg@10', 'hr@50']:\n",
    "        v = r.get(k, 0)\n",
    "        s = r.get(k + '_std', 0)\n",
    "        parts.append(f'{k}: {v:.4f}+/-{s:.3f}')\n",
    "    print(f'  {m:<22s} {\" | \".join(parts)}')\n",
    "print()\n",
    "pm = PAPER['MARec (paper)']\n",
    "best_key = '+CA+UA+GE-Rec' if '+CA+UA+GE-Rec' in abl_summary else 'MARec Baseline'\n",
    "best = abl_summary[best_key]\n",
    "for m in ['hr@10', 'ndcg@10']:\n",
    "    ours = best.get(m, 0)\n",
    "    paper = pm[m]\n",
    "    d = (ours - paper) / paper * 100\n",
    "    print(f'  {best_key} {m}: {ours:.4f} vs paper {paper:.4f} ({d:+.1f}%)')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Enhancements Summary\n",
    "\n",
    "| Model | Addresses | Mechanism |\n",
    "|-------|-----------|-----------|\n",
    "| **CA-Rec** | Linearity, collapse | InfoNCE contrastive loss: pulls matching, pushes non-matching |\n",
    "| **UA-Rec** | Metadata noise | Gaussian NLL: sigma^2 down-weights noisy/sparse features |\n",
    "| **GE-Rec** | Bland proxies | CVAE: learns distribution of interaction embeddings, sharp proxies |\n",
    "\n",
    "**MARec Limitations from Slide 12:**\n",
    "1. Linearity Assumption -> CA-Rec + GE-Rec add non-linear mappings\n",
    "2. Metadata Quality Dependency -> UA-Rec down-weights noisy features automatically\n",
    "3. Not Fully End-to-End -> GE-Rec learns the distribution of embeddings\n",
    "4. Domain Shift -> CA-Rec contrastive objective is more robust\n",
    "\n",
    "**Runtime config:** 3 splits, 2 seeds, 15/15/20 epochs, 128-dim hidden -> ~20 min on T4"
   ]
  }
 ]
}